{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_communities = [\"r/Science\", \"r/changemyview\", \"r/AmItheAsshole\"]\n",
    "API_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "data_dir = \"./raw_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: 50% of the performace of a post is based off of upvote score, 50% of the performance of a post is based on number of comments for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/Science  2025-10\n",
      "  saved 50 → ./raw_data/Science/2025-10_top50.csv\n",
      "r/Science  2025-09\n",
      "  saved 50 → ./raw_data/Science/2025-09_top50.csv\n",
      "r/Science  2025-08\n",
      "  saved 50 → ./raw_data/Science/2025-08_top50.csv\n",
      "r/Science  2025-07\n",
      "  saved 50 → ./raw_data/Science/2025-07_top50.csv\n",
      "r/Science  2025-06\n",
      "  saved 50 → ./raw_data/Science/2025-06_top50.csv\n",
      "r/Science  2025-05\n",
      "  saved 50 → ./raw_data/Science/2025-05_top50.csv\n",
      "r/Science  2025-04\n",
      "  saved 50 → ./raw_data/Science/2025-04_top50.csv\n",
      "r/Science  2025-03\n",
      "  saved 50 → ./raw_data/Science/2025-03_top50.csv\n",
      "r/Science  2025-02\n",
      "  saved 50 → ./raw_data/Science/2025-02_top50.csv\n",
      "r/Science  2025-01\n",
      "  saved 50 → ./raw_data/Science/2025-01_top50.csv\n",
      "r/Science  2024-12\n",
      "  saved 50 → ./raw_data/Science/2024-12_top50.csv\n",
      "r/Science  2024-11\n",
      "  saved 50 → ./raw_data/Science/2024-11_top50.csv\n",
      "r/Science  2024-10\n",
      "  saved 50 → ./raw_data/Science/2024-10_top50.csv\n",
      "r/Science  2024-09\n",
      "  saved 50 → ./raw_data/Science/2024-09_top50.csv\n",
      "r/Science  2024-08\n",
      "  saved 50 → ./raw_data/Science/2024-08_top50.csv\n",
      "r/Science  2024-07\n",
      "  saved 50 → ./raw_data/Science/2024-07_top50.csv\n",
      "r/Science  2024-06\n",
      "  saved 50 → ./raw_data/Science/2024-06_top50.csv\n",
      "r/Science  2024-05\n",
      "  saved 50 → ./raw_data/Science/2024-05_top50.csv\n",
      "r/Science  2024-04\n",
      "  saved 50 → ./raw_data/Science/2024-04_top50.csv\n",
      "r/Science  2024-03\n",
      "  saved 50 → ./raw_data/Science/2024-03_top50.csv\n",
      "r/Science  2024-02\n",
      "  saved 50 → ./raw_data/Science/2024-02_top50.csv\n",
      "r/Science  2024-01\n",
      "  saved 50 → ./raw_data/Science/2024-01_top50.csv\n",
      "r/Science  2023-12\n",
      "  saved 50 → ./raw_data/Science/2023-12_top50.csv\n",
      "r/Science  2023-11\n",
      "  saved 50 → ./raw_data/Science/2023-11_top50.csv\n",
      "r/changemyview  2025-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  saved 50 → ./raw_data/changemyview/2025-10_top50.csv\n",
      "r/changemyview  2025-09\n",
      "  saved 50 → ./raw_data/changemyview/2025-09_top50.csv\n",
      "r/changemyview  2025-08\n",
      "  saved 50 → ./raw_data/changemyview/2025-08_top50.csv\n",
      "r/changemyview  2025-07\n",
      "  saved 50 → ./raw_data/changemyview/2025-07_top50.csv\n",
      "r/changemyview  2025-06\n",
      "  saved 50 → ./raw_data/changemyview/2025-06_top50.csv\n",
      "r/changemyview  2025-05\n",
      "  saved 50 → ./raw_data/changemyview/2025-05_top50.csv\n",
      "r/changemyview  2025-04\n",
      "  saved 50 → ./raw_data/changemyview/2025-04_top50.csv\n",
      "r/changemyview  2025-03\n",
      "  saved 50 → ./raw_data/changemyview/2025-03_top50.csv\n",
      "r/changemyview  2025-02\n",
      "  saved 50 → ./raw_data/changemyview/2025-02_top50.csv\n",
      "r/changemyview  2025-01\n",
      "  saved 50 → ./raw_data/changemyview/2025-01_top50.csv\n",
      "r/changemyview  2024-12\n",
      "  saved 50 → ./raw_data/changemyview/2024-12_top50.csv\n",
      "r/changemyview  2024-11\n",
      "  saved 50 → ./raw_data/changemyview/2024-11_top50.csv\n",
      "r/changemyview  2024-10\n",
      "  saved 50 → ./raw_data/changemyview/2024-10_top50.csv\n",
      "r/changemyview  2024-09\n",
      "  saved 50 → ./raw_data/changemyview/2024-09_top50.csv\n",
      "r/changemyview  2024-08\n",
      "  saved 50 → ./raw_data/changemyview/2024-08_top50.csv\n",
      "r/changemyview  2024-07\n",
      "  saved 50 → ./raw_data/changemyview/2024-07_top50.csv\n",
      "r/changemyview  2024-06\n",
      "  saved 50 → ./raw_data/changemyview/2024-06_top50.csv\n",
      "r/changemyview  2024-05\n",
      "  saved 50 → ./raw_data/changemyview/2024-05_top50.csv\n",
      "r/changemyview  2024-04\n",
      "  saved 50 → ./raw_data/changemyview/2024-04_top50.csv\n",
      "r/changemyview  2024-03\n",
      "  saved 50 → ./raw_data/changemyview/2024-03_top50.csv\n",
      "r/changemyview  2024-02\n",
      "  saved 50 → ./raw_data/changemyview/2024-02_top50.csv\n",
      "r/changemyview  2024-01\n",
      "  saved 50 → ./raw_data/changemyview/2024-01_top50.csv\n",
      "r/changemyview  2023-12\n",
      "  saved 50 → ./raw_data/changemyview/2023-12_top50.csv\n",
      "r/changemyview  2023-11\n",
      "  saved 50 → ./raw_data/changemyview/2023-11_top50.csv\n",
      "r/AmItheAsshole  2025-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  saved 50 → ./raw_data/AmItheAsshole/2025-10_top50.csv\n",
      "r/AmItheAsshole  2025-09\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-09_top50.csv\n",
      "r/AmItheAsshole  2025-08\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-08_top50.csv\n",
      "r/AmItheAsshole  2025-07\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-07_top50.csv\n",
      "r/AmItheAsshole  2025-06\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-06_top50.csv\n",
      "r/AmItheAsshole  2025-05\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-05_top50.csv\n",
      "r/AmItheAsshole  2025-04\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-04_top50.csv\n",
      "r/AmItheAsshole  2025-03\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-03_top50.csv\n",
      "r/AmItheAsshole  2025-02\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-02_top50.csv\n",
      "r/AmItheAsshole  2025-01\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2025-01_top50.csv\n",
      "r/AmItheAsshole  2024-12\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-12_top50.csv\n",
      "r/AmItheAsshole  2024-11\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-11_top50.csv\n",
      "r/AmItheAsshole  2024-10\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-10_top50.csv\n",
      "r/AmItheAsshole  2024-09\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-09_top50.csv\n",
      "r/AmItheAsshole  2024-08\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-08_top50.csv\n",
      "r/AmItheAsshole  2024-07\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-07_top50.csv\n",
      "r/AmItheAsshole  2024-06\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-06_top50.csv\n",
      "r/AmItheAsshole  2024-05\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-05_top50.csv\n",
      "r/AmItheAsshole  2024-04\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-04_top50.csv\n",
      "r/AmItheAsshole  2024-03\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-03_top50.csv\n",
      "r/AmItheAsshole  2024-02\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-02_top50.csv\n",
      "r/AmItheAsshole  2024-01\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2024-01_top50.csv\n",
      "r/AmItheAsshole  2023-12\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2023-12_top50.csv\n",
      "r/AmItheAsshole  2023-11\n",
      "  saved 50 → ./raw_data/AmItheAsshole/2023-11_top50.csv\n"
     ]
    }
   ],
   "source": [
    "def fetch_posts(subreddit, after, before, limit=100):\n",
    "    params = {\n",
    "        \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "        \"after\": after, \"before\": before,\n",
    "        \"limit\": limit, \"sort\": \"desc\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(API_URL, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch err] {subreddit} {after}→{before}: {e}\")\n",
    "        return []\n",
    "\n",
    "def minmax_norm(series):\n",
    "    smin, smax = series.min(), series.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series([0.5] * len(series), index=series.index)  # flat month → neutral 0.5\n",
    "    return (series - smin) / (smax - smin)\n",
    "\n",
    "def collect_month(subreddit, year, month):\n",
    "    # month bounds\n",
    "    start = datetime(year, month, 1)\n",
    "    end   = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "    after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    seen, rows, cursor = set(), [], before\n",
    "    while True:\n",
    "        batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "        if not batch:\n",
    "            break\n",
    "        for p in batch:\n",
    "            pid = p.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen.add(pid)\n",
    "                rows.append(p)\n",
    "        oldest_ts = min(p.get(\"created_utc\", 0) for p in batch)\n",
    "        cursor = datetime.fromtimestamp(oldest_ts).strftime(\"%Y-%m-%d\")\n",
    "        if cursor <= after:\n",
    "            break\n",
    "        time.sleep(1)  # be nice to the API\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "    # 50/50 popularity = normalized score (50%) + normalized comments (50%)\n",
    "    if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "        df[\"score_norm\"] = minmax_norm(df[\"score\"].astype(\"float\"))\n",
    "        df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].astype(\"float\"))\n",
    "        df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "        df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "    else:\n",
    "        # fallback: if fields missing, keep top 50 by score\n",
    "        df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def last_n_months(n=24):\n",
    "    # step back month-by-month from the first of the current month\n",
    "    anchor = datetime.utcnow().replace(day=1)\n",
    "    y, m = anchor.year, anchor.month\n",
    "    for _ in range(n):\n",
    "        yield y, m\n",
    "        m -= 1\n",
    "        if m == 0:\n",
    "            m, y = 12, y - 1\n",
    "\n",
    "for sub in subreddit_communities:\n",
    "    sub_name = sub.replace(\"r/\", \"\")\n",
    "    out_dir = os.path.join(data_dir, sub_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for y, m in last_n_months(24):\n",
    "        print(f\"{sub}  {y}-{m:02d}\")\n",
    "        dfm = collect_month(sub, y, m)\n",
    "        if dfm.empty:\n",
    "            print(\"  (no data)\")\n",
    "            continue\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "        dfm[[\n",
    "            \"id\",\"title\",\"selftext\",\"author\",\"score\",\"num_comments\",\"upvote_ratio\",\n",
    "            \"popularity\",\"created_utc\",\"permalink\",\"year\",\"month\"\n",
    "        ]].to_csv(out_path, index=False)\n",
    "        print(f\"  saved {len(dfm)} → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/Science  2023-09\n",
      "  saved 50 → ./raw_25month_range_data/Science/2023-09_top50.csv\n",
      "r/Science  2023-10\n",
      "  saved 50 → ./raw_25month_range_data/Science/2023-10_top50.csv\n",
      "r/Science  2023-11\n",
      "  saved 50 → ./raw_25month_range_data/Science/2023-11_top50.csv\n",
      "r/Science  2023-12\n",
      "  saved 50 → ./raw_25month_range_data/Science/2023-12_top50.csv\n",
      "r/Science  2024-01\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-01_top50.csv\n",
      "r/Science  2024-02\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-02_top50.csv\n",
      "r/Science  2024-03\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-03_top50.csv\n",
      "r/Science  2024-04\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-04_top50.csv\n",
      "r/Science  2024-05\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-05_top50.csv\n",
      "r/Science  2024-06\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-06_top50.csv\n",
      "r/Science  2024-07\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-07_top50.csv\n",
      "r/Science  2024-08\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-08_top50.csv\n",
      "r/Science  2024-09\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-09_top50.csv\n",
      "r/Science  2024-10\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-10_top50.csv\n",
      "r/Science  2024-11\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-11_top50.csv\n",
      "r/Science  2024-12\n",
      "  saved 50 → ./raw_25month_range_data/Science/2024-12_top50.csv\n",
      "r/Science  2025-01\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-01_top50.csv\n",
      "r/Science  2025-02\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-02_top50.csv\n",
      "r/Science  2025-03\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-03_top50.csv\n",
      "r/Science  2025-04\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-04_top50.csv\n",
      "r/Science  2025-05\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-05_top50.csv\n",
      "r/Science  2025-06\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-06_top50.csv\n",
      "r/Science  2025-07\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-07_top50.csv\n",
      "r/Science  2025-08\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-08_top50.csv\n",
      "r/Science  2025-09\n",
      "  saved 50 → ./raw_25month_range_data/Science/2025-09_top50.csv\n",
      "r/changemyview  2023-09\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2023-09_top50.csv\n",
      "r/changemyview  2023-10\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2023-10_top50.csv\n",
      "r/changemyview  2023-11\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2023-11_top50.csv\n",
      "r/changemyview  2023-12\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2023-12_top50.csv\n",
      "r/changemyview  2024-01\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-01_top50.csv\n",
      "r/changemyview  2024-02\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-02_top50.csv\n",
      "r/changemyview  2024-03\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-03_top50.csv\n",
      "r/changemyview  2024-04\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-04_top50.csv\n",
      "r/changemyview  2024-05\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-05_top50.csv\n",
      "r/changemyview  2024-06\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-06_top50.csv\n",
      "r/changemyview  2024-07\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-07_top50.csv\n",
      "r/changemyview  2024-08\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-08_top50.csv\n",
      "r/changemyview  2024-09\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-09_top50.csv\n",
      "r/changemyview  2024-10\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-10_top50.csv\n",
      "r/changemyview  2024-11\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-11_top50.csv\n",
      "r/changemyview  2024-12\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2024-12_top50.csv\n",
      "r/changemyview  2025-01\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-01_top50.csv\n",
      "r/changemyview  2025-02\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-02_top50.csv\n",
      "r/changemyview  2025-03\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-03_top50.csv\n",
      "r/changemyview  2025-04\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-04_top50.csv\n",
      "r/changemyview  2025-05\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-05_top50.csv\n",
      "r/changemyview  2025-06\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-06_top50.csv\n",
      "r/changemyview  2025-07\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-07_top50.csv\n",
      "r/changemyview  2025-08\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-08_top50.csv\n",
      "r/changemyview  2025-09\n",
      "  saved 50 → ./raw_25month_range_data/changemyview/2025-09_top50.csv\n",
      "r/AmItheAsshole  2023-09\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2023-09_top50.csv\n",
      "r/AmItheAsshole  2023-10\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2023-10_top50.csv\n",
      "r/AmItheAsshole  2023-11\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2023-11_top50.csv\n",
      "r/AmItheAsshole  2023-12\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2023-12_top50.csv\n",
      "r/AmItheAsshole  2024-01\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-01_top50.csv\n",
      "r/AmItheAsshole  2024-02\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-02_top50.csv\n",
      "r/AmItheAsshole  2024-03\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-03_top50.csv\n",
      "r/AmItheAsshole  2024-04\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-04_top50.csv\n",
      "r/AmItheAsshole  2024-05\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-05_top50.csv\n",
      "r/AmItheAsshole  2024-06\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-06_top50.csv\n",
      "r/AmItheAsshole  2024-07\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-07_top50.csv\n",
      "r/AmItheAsshole  2024-08\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-08_top50.csv\n",
      "r/AmItheAsshole  2024-09\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-09_top50.csv\n",
      "r/AmItheAsshole  2024-10\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-10_top50.csv\n",
      "r/AmItheAsshole  2024-11\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-11_top50.csv\n",
      "r/AmItheAsshole  2024-12\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2024-12_top50.csv\n",
      "r/AmItheAsshole  2025-01\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-01_top50.csv\n",
      "r/AmItheAsshole  2025-02\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-02_top50.csv\n",
      "r/AmItheAsshole  2025-03\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-03_top50.csv\n",
      "r/AmItheAsshole  2025-04\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-04_top50.csv\n",
      "r/AmItheAsshole  2025-05\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-05_top50.csv\n",
      "r/AmItheAsshole  2025-06\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-06_top50.csv\n",
      "r/AmItheAsshole  2025-07\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-07_top50.csv\n",
      "r/AmItheAsshole  2025-08\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-08_top50.csv\n",
      "r/AmItheAsshole  2025-09\n",
      "  saved 50 → ./raw_25month_range_data/AmItheAsshole/2025-09_top50.csv\n"
     ]
    }
   ],
   "source": [
    "import os, glob, time, requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "subreddit_communities = [\"r/Science\", \"r/changemyview\", \"r/AmItheAsshole\"]\n",
    "API_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "data_dir = \"./raw_25month_range_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def fetch_posts(subreddit, after, before, limit=100):\n",
    "    params = {\n",
    "        \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "        \"after\": after,\n",
    "        \"before\": before,\n",
    "        \"limit\": limit,\n",
    "        \"sort\": \"desc\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(API_URL, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch err] {subreddit} {after}→{before}: {e}\")\n",
    "        return []\n",
    "\n",
    "def minmax_norm(series):\n",
    "    smin, smax = series.min(), series.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series([0.5] * len(series), index=series.index)\n",
    "    return (series - smin) / (smax - smin)\n",
    "\n",
    "def collect_month(subreddit, year, month):\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "    after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    seen, rows, cursor = set(), [], before\n",
    "    while True:\n",
    "        batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        for p in batch:\n",
    "            pid = p.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen.add(pid)\n",
    "                rows.append(p)\n",
    "\n",
    "        oldest_ts = min((p.get(\"created_utc\", 0) or 0) for p in batch)\n",
    "        if not oldest_ts:\n",
    "            break\n",
    "        cursor = datetime.fromtimestamp(float(oldest_ts)).strftime(\"%Y-%m-%d\")\n",
    "        if cursor <= after:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "    # popularity = 0.5 * norm(score) + 0.5 * norm(num_comments)\n",
    "    if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "        df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "        df[\"num_comments\"] = pd.to_numeric(df[\"num_comments\"], errors=\"coerce\")\n",
    "        df[\"score_norm\"] = minmax_norm(df[\"score\"].fillna(0))\n",
    "        df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].fillna(0))\n",
    "        df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "        df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "    else:\n",
    "        # fallback if fields missing\n",
    "        if \"score\" in df.columns:\n",
    "            df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").fillna(0)\n",
    "            df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.head(50).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def months_range(start_year, start_month, end_year, end_month):\n",
    "    y, m = start_year, start_month\n",
    "    while (y < end_year) or (y == end_year and m <= end_month):\n",
    "        yield y, m\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            y += 1\n",
    "            m = 1\n",
    "\n",
    "# Inclusive window: 2023-09 through 2025-09 (NOTE: this is 25 months total)\n",
    "START_Y, START_M = 2023, 9\n",
    "END_Y, END_M = 2025, 9\n",
    "\n",
    "for sub in subreddit_communities:\n",
    "    sub_name = sub.replace(\"r/\", \"\")\n",
    "    out_dir = os.path.join(data_dir, sub_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for y, m in months_range(START_Y, START_M, END_Y, END_M):\n",
    "        print(f\"{sub}  {y}-{m:02d}\")\n",
    "        dfm = collect_month(sub, y, m)\n",
    "        if dfm.empty:\n",
    "            print(\"  (no data)\")\n",
    "            continue\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "        cols = [\n",
    "            \"id\", \"title\", \"selftext\", \"author\", \"score\", \"num_comments\",\n",
    "            \"upvote_ratio\", \"popularity\", \"created_utc\", \"permalink\", \"year\", \"month\"\n",
    "        ]\n",
    "        keep = [c for c in cols if c in dfm.columns]\n",
    "        dfm[keep].to_csv(out_path, index=False)\n",
    "        print(f\"  saved {len(dfm)} → {out_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to train per-subreddit community, so create a Pandas dataframe (and CSV file) for each subreddit community that we can train our model based off of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved → ./combined_raw_25month_range_data/Science_combined_24mo.csv (1250 rows)\n",
      "saved → ./combined_raw_25month_range_data/changemyview_combined_24mo.csv (1250 rows)\n",
      "saved → ./combined_raw_25month_range_data/AmItheAsshole_combined_24mo.csv (1250 rows)\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = \"./raw_25month_range_data\"\n",
    "output_data_dir = \"./combined_raw_25month_range_data\"\n",
    "os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "def load_sub(sub):\n",
    "    files = sorted(glob.glob(os.path.join(raw_data_dir, sub, \"*.csv\")))\n",
    "    if not files:\n",
    "        print(f\"(no csvs) {sub}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat((pd.read_csv(f, low_memory=False) for f in files), ignore_index=True)\n",
    "    if \"subreddit\" not in df.columns:\n",
    "        df[\"subreddit\"] = sub\n",
    "    if \"id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=\"id\")\n",
    "    if \"created_utc\" in df.columns and \"created_datetime\" not in df.columns:\n",
    "        df[\"created_utc\"] = pd.to_numeric(df[\"created_utc\"], errors=\"coerce\")\n",
    "        df[\"created_datetime\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# build one DataFrame per subreddit\n",
    "df_science       = load_sub(\"Science\")\n",
    "df_changemyview  = load_sub(\"changemyview\")\n",
    "df_amitheasshole = load_sub(\"AmItheAsshole\")\n",
    "\n",
    "# save each to ./combined_subreddit_raw_data/\n",
    "outputs = {\n",
    "    \"Science\": df_science,\n",
    "    \"changemyview\": df_changemyview,\n",
    "    \"AmItheAsshole\": df_amitheasshole,\n",
    "}\n",
    "for name, df in outputs.items():\n",
    "    if df.empty:\n",
    "        print(f\"(skip empty) {name}\")\n",
    "        continue\n",
    "    out_path = os.path.join(output_data_dir, f\"{name}_combined_24mo.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"saved → {out_path} ({len(df)} rows)\")\n",
    "    \n",
    "# accounts for current month so collective of 25 months of data\n",
    "# currently, voting score and number of comments on a post have the same engagement weight (50%) - we can potentially determine better, more effecte weights "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
