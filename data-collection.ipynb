{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_communities = [\"r/Science\", \"r/changemyview\", \"r/AmItheAsshole\"]\n",
    "API_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "data_dir = \"./raw_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "def fetch_posts_batch(subreddit_name, after_date, before_date, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch a single batch of posts using the Arctic Shift API \n",
    "    Params:\n",
    "    - subreddit_name: str, name of the subreddit\n",
    "    - after_date: str, 'YYYY-MM-DD' format\n",
    "    - before_date: str, 'YYYY-MM-DD' format\n",
    "    - limit: int, number of posts to fetch at once (default 100)\n",
    "    \"\"\"\n",
    "    url = \"https://arctic-shift.photon-reddit.com/api/posts/search\" # endpoint, see docs @ https://github.com/ArthurHeitmann/arctic_shift/tree/master/api\n",
    "    \n",
    "    params = {\n",
    "        'subreddit': subreddit_name,\n",
    "        'after': after_date,\n",
    "        'before': before_date,\n",
    "        'limit': limit,\n",
    "        'sort': 'desc'  # newest first\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if data and 'data' in data:\n",
    "            return data['data']\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching batch: {e}\")\n",
    "        return []\n",
    "\n",
    "def collect_posts(subreddit_name, month, year):\n",
    "    \"\"\"\n",
    "    Collect all posts from a month recursively from newest to oldest in batches\n",
    "    Uses `fetch_posts_batch` to get posts \n",
    "    Params:\n",
    "    - subreddit_name: str, name of the subreddit\n",
    "    - month: int, month number (1-12)\n",
    "    - year: int, year (e.g., 2023)\n",
    "    \"\"\"\n",
    "    print(f\"Collecting posts from r/{subreddit_name} for {calendar.month_name[month]} {year}\")\n",
    "    \n",
    "    # convert month to boundaries\n",
    "    start_date = datetime(year, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(year + 1, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(year, month + 1, 1)\n",
    "    \n",
    "    original_after = start_date.strftime('%Y-%m-%d')\n",
    "    current_before = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    all_posts = []\n",
    "    seen_post_ids = set()\n",
    "    batch_num = 1\n",
    "\n",
    "    # recursively collect posts until we reach the end of the month     \n",
    "    while True:\n",
    "        print(f\"Batch {batch_num}: {original_after} to {current_before}\")\n",
    "        \n",
    "        # fetch batch of 100 posts\n",
    "        batch_posts = fetch_posts_batch(subreddit_name, original_after, current_before, 100)\n",
    "        \n",
    "        if not batch_posts:\n",
    "            print(f\"No more posts available\")\n",
    "            break\n",
    "        \n",
    "        # filter out dupes, add new posts\n",
    "        new_posts = []\n",
    "        for post in batch_posts:\n",
    "            post_id = post.get('id')\n",
    "            if post_id and post_id not in seen_post_ids:\n",
    "                seen_post_ids.add(post_id)\n",
    "                new_posts.append(post)\n",
    "        \n",
    "        if not new_posts:\n",
    "            print(f\"No new posts in batch (all duplicates)\")\n",
    "            break\n",
    "        \n",
    "        all_posts.extend(new_posts)\n",
    "        print(f\"Added {len(new_posts)} new posts (total: {len(all_posts)})\")\n",
    "        \n",
    "        if len(batch_posts) < 100:\n",
    "            print(f\" Incomplete batch ({len(batch_posts)} posts) - reached end\")\n",
    "            break\n",
    "        \n",
    "        # update timestamp of the oldest post in the fetched batch\n",
    "        oldest_timestamp = min(post.get('created_utc', 0) for post in batch_posts)\n",
    "        current_before = datetime.fromtimestamp(oldest_timestamp).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # ensure we don't go past the month \n",
    "        if current_before <= original_after:\n",
    "            print(f\"Reached date boundary\")\n",
    "            break\n",
    "        \n",
    "        batch_num += 1\n",
    "        time.sleep(1)  # rate limiting by calling sleep for one second between calls\n",
    "    \n",
    "    print(f\"Total unique posts collected: {len(all_posts)}\")\n",
    "    return all_posts\n",
    "\n",
    "def process_posts_to_dataframe(raw_posts, subreddit_name, month, year):\n",
    "    \"\"\"\n",
    "    Convert raw post data to DataFrame for storage and ease of future processing\n",
    "    Params:\n",
    "    - raw_posts: list of dicts, raw post data from Arctic Shift\n",
    "    - subreddit_name: str, name of the subreddit\n",
    "    - month: int, month number (1-12)\n",
    "    - year: int, year (e.g., 2023)\n",
    "    \"\"\"\n",
    "    posts_data = []\n",
    "    for post in raw_posts:\n",
    "        if not post.get('id') or not post.get('created_utc'):\n",
    "            continue\n",
    "        \n",
    "        # generate permalink properly\n",
    "        permalink = post.get('permalink', '')\n",
    "        if permalink and not permalink.startswith('http'):\n",
    "            # add prefix\n",
    "            if permalink.startswith('/r/'):\n",
    "                full_permalink = f\"https://www.reddit.com{permalink}\"\n",
    "            else:\n",
    "                full_permalink = f\"https://www.reddit.com/r/{subreddit_name}/comments/{post.get('id', '')}\"\n",
    "        else:\n",
    "            full_permalink = permalink\n",
    "\n",
    "        # ignore fully external posts (no selftext)\n",
    "        if not post.get('selftext'):\n",
    "            continue\n",
    "\n",
    "        # structure fields from the post into DataFrame            \n",
    "        posts_data.append({\n",
    "            'post_id': post.get('id'),\n",
    "            'title': post.get('title', ''),\n",
    "            'text': post.get('selftext', ''),\n",
    "            'author': post.get('author', '[deleted]'),\n",
    "            'score': post.get('score', 0),\n",
    "            'upvote_ratio': post.get('upvote_ratio', 0),\n",
    "            'num_comments': post.get('num_comments', 0),\n",
    "            'created_utc': post.get('created_utc'),\n",
    "            'created_datetime': datetime.fromtimestamp(post.get('created_utc', 0)),\n",
    "            'subreddit': subreddit_name,\n",
    "            'permalink': full_permalink,  \n",
    "            'month': month,\n",
    "            'year': year\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(posts_data)\n",
    "\n",
    "def get_top_n_posts(df, n=10):\n",
    "    \"\"\"\n",
    "    Sort posts in a DataFrame by score and return top n\n",
    "    Params:\n",
    "    - df: pd.DataFrame, DataFrame of posts\n",
    "    - n: int, number of top posts to return (default 10)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # sort by score (descending) and get top n\n",
    "    top_posts = df.sort_values('score', ascending=False).head(n).reset_index(drop=True)\n",
    "    \n",
    "    if not top_posts.empty:\n",
    "        score_range = f\"{top_posts['score'].max()} to {top_posts['score'].min()}\"\n",
    "        print(f\"Score range: {score_range}\")\n",
    "    \n",
    "    return top_posts\n",
    "\n",
    "def get_top_posts_arctic_shift(subreddit_name, month, year, n=10):\n",
    "    \"\"\"\n",
    "    Using all our helpers to get top n posts from a subreddit in a specific month using Arctic Shift API\n",
    "    Params:\n",
    "    - subreddit_name: str, name of the subreddit\n",
    "    - month: int, month number (1-12)\n",
    "    - year: int, year (e.g., 2023)\n",
    "    - n: int, number of top posts to return (default 10)\n",
    "    \"\"\"    \n",
    "    raw_posts = collect_posts(subreddit_name, month, year)\n",
    "    \n",
    "    if not raw_posts:\n",
    "        print(\"No posts collected\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = process_posts_to_dataframe(raw_posts, subreddit_name, month, year)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No valid posts processed\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    top_posts = get_top_n_posts(df, n)\n",
    "    \n",
    "    print(f\"Collected {len(raw_posts)} total posts from r/{subreddit_name} for {calendar.month_name[month]} {year}\")\n",
    "    \n",
    "    return top_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/science for August 2023\n",
      "Batch 1: 2023-08-01 to 2023-09-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2023-08-01 to 2023-08-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2023-08-01 to 2023-08-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2023-08-01 to 2023-08-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2023-08-01 to 2023-08-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2023-08-01 to 2023-08-21\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2023-08-01 to 2023-08-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2023-08-01 to 2023-08-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2023-08-01 to 2023-08-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2023-08-01 to 2023-08-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2023-08-01 to 2023-08-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2023-08-01 to 2023-08-07\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2023-08-01 to 2023-08-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2023-08-01 to 2023-08-02\n",
      "Added 63 new posts (total: 1363)\n",
      " Incomplete batch (63 posts) - reached end\n",
      "Total unique posts collected: 1363\n",
      "Score range: 178 to 1\n",
      "Collected 1363 total posts from r/science for August 2023\n",
      "===Saved top posts to data/information_science_2023_08.csv===\n",
      "Collecting posts from r/science for September 2023\n",
      "Batch 1: 2023-09-01 to 2023-10-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2023-09-01 to 2023-09-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2023-09-01 to 2023-09-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2023-09-01 to 2023-09-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2023-09-01 to 2023-09-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2023-09-01 to 2023-09-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2023-09-01 to 2023-09-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2023-09-01 to 2023-09-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2023-09-01 to 2023-09-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2023-09-01 to 2023-09-09\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2023-09-01 to 2023-09-07\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2023-09-01 to 2023-09-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2023-09-01 to 2023-09-02\n",
      "Added 86 new posts (total: 1286)\n",
      " Incomplete batch (86 posts) - reached end\n",
      "Total unique posts collected: 1286\n",
      "Score range: 1232 to 1\n",
      "Collected 1286 total posts from r/science for September 2023\n",
      "===Saved top posts to data/information_science_2023_09.csv===\n",
      "Collecting posts from r/science for October 2023\n",
      "Batch 1: 2023-10-01 to 2023-11-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2023-10-01 to 2023-10-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2023-10-01 to 2023-10-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2023-10-01 to 2023-10-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2023-10-01 to 2023-10-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2023-10-01 to 2023-10-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2023-10-01 to 2023-10-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2023-10-01 to 2023-10-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2023-10-01 to 2023-10-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2023-10-01 to 2023-10-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2023-10-01 to 2023-10-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2023-10-01 to 2023-10-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2023-10-01 to 2023-10-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2023-10-01 to 2023-10-02\n",
      "Added 100 new posts (total: 1400)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1400\n",
      "Score range: 212 to 1\n",
      "Collected 1400 total posts from r/science for October 2023\n",
      "===Saved top posts to data/information_science_2023_10.csv===\n",
      "Collecting posts from r/science for November 2023\n",
      "Batch 1: 2023-11-01 to 2023-12-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2023-11-01 to 2023-11-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2023-11-01 to 2023-11-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2023-11-01 to 2023-11-23\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2023-11-01 to 2023-11-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2023-11-01 to 2023-11-18\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2023-11-01 to 2023-11-16\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2023-11-01 to 2023-11-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2023-11-01 to 2023-11-11\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2023-11-01 to 2023-11-08\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2023-11-01 to 2023-11-06\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2023-11-01 to 2023-11-03\n",
      "Added 100 new posts (total: 1200)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1200\n",
      "Score range: 1 to 1\n",
      "Collected 1200 total posts from r/science for November 2023\n",
      "===Saved top posts to data/information_science_2023_11.csv===\n",
      "Collecting posts from r/science for December 2023\n",
      "Batch 1: 2023-12-01 to 2024-01-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2023-12-01 to 2023-12-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2023-12-01 to 2023-12-25\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2023-12-01 to 2023-12-22\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2023-12-01 to 2023-12-20\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2023-12-01 to 2023-12-18\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2023-12-01 to 2023-12-15\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2023-12-01 to 2023-12-13\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2023-12-01 to 2023-12-11\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2023-12-01 to 2023-12-08\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2023-12-01 to 2023-12-06\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2023-12-01 to 2023-12-03\n",
      "Added 100 new posts (total: 1200)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1200\n",
      "Score range: 1 to 1\n",
      "Collected 1200 total posts from r/science for December 2023\n",
      "===Saved top posts to data/information_science_2023_12.csv===\n",
      "Collecting posts from r/science for January 2024\n",
      "Batch 1: 2024-01-01 to 2024-02-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-01-01 to 2024-01-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-01-01 to 2024-01-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-01-01 to 2024-01-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-01-01 to 2024-01-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-01-01 to 2024-01-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-01-01 to 2024-01-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-01-01 to 2024-01-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-01-01 to 2024-01-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-01-01 to 2024-01-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-01-01 to 2024-01-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-01-01 to 2024-01-07\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-01-01 to 2024-01-05\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2024-01-01 to 2024-01-03\n",
      "Added 88 new posts (total: 1388)\n",
      " Incomplete batch (88 posts) - reached end\n",
      "Total unique posts collected: 1388\n",
      "Score range: 1 to 1\n",
      "Collected 1388 total posts from r/science for January 2024\n",
      "===Saved top posts to data/information_science_2024_01.csv===\n",
      "Collecting posts from r/science for February 2024\n",
      "Batch 1: 2024-02-01 to 2024-03-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-02-01 to 2024-02-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-02-01 to 2024-02-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-02-01 to 2024-02-23\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-02-01 to 2024-02-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-02-01 to 2024-02-18\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-02-01 to 2024-02-15\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-02-01 to 2024-02-12\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-02-01 to 2024-02-09\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-02-01 to 2024-02-07\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-02-01 to 2024-02-05\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-02-01 to 2024-02-02\n",
      "Added 55 new posts (total: 1155)\n",
      " Incomplete batch (55 posts) - reached end\n",
      "Total unique posts collected: 1155\n",
      "Score range: 1 to 1\n",
      "Collected 1155 total posts from r/science for February 2024\n",
      "===Saved top posts to data/information_science_2024_02.csv===\n",
      "Collecting posts from r/science for March 2024\n",
      "Batch 1: 2024-03-01 to 2024-04-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-03-01 to 2024-03-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-03-01 to 2024-03-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-03-01 to 2024-03-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-03-01 to 2024-03-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-03-01 to 2024-03-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-03-01 to 2024-03-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-03-01 to 2024-03-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-03-01 to 2024-03-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-03-01 to 2024-03-09\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-03-01 to 2024-03-06\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-03-01 to 2024-03-04\n",
      "Added 100 new posts (total: 1200)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1200\n",
      "Score range: 2 to 1\n",
      "Collected 1200 total posts from r/science for March 2024\n",
      "===Saved top posts to data/information_science_2024_03.csv===\n",
      "Collecting posts from r/science for April 2024\n",
      "Batch 1: 2024-04-01 to 2024-05-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-04-01 to 2024-04-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-04-01 to 2024-04-25\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-04-01 to 2024-04-23\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-04-01 to 2024-04-20\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-04-01 to 2024-04-18\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-04-01 to 2024-04-16\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-04-01 to 2024-04-13\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-04-01 to 2024-04-11\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-04-01 to 2024-04-09\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-04-01 to 2024-04-06\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-04-01 to 2024-04-04\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-04-01 to 2024-04-02\n",
      "Added 47 new posts (total: 1247)\n",
      " Incomplete batch (47 posts) - reached end\n",
      "Total unique posts collected: 1247\n",
      "Score range: 69 to 1\n",
      "Collected 1247 total posts from r/science for April 2024\n",
      "===Saved top posts to data/information_science_2024_04.csv===\n",
      "Collecting posts from r/science for May 2024\n",
      "Batch 1: 2024-05-01 to 2024-06-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-05-01 to 2024-05-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-05-01 to 2024-05-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-05-01 to 2024-05-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-05-01 to 2024-05-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-05-01 to 2024-05-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-05-01 to 2024-05-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-05-01 to 2024-05-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-05-01 to 2024-05-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-05-01 to 2024-05-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-05-01 to 2024-05-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-05-01 to 2024-05-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-05-01 to 2024-05-02\n",
      "Added 47 new posts (total: 1247)\n",
      " Incomplete batch (47 posts) - reached end\n",
      "Total unique posts collected: 1247\n",
      "Score range: 1 to 1\n",
      "Collected 1247 total posts from r/science for May 2024\n",
      "===Saved top posts to data/information_science_2024_05.csv===\n",
      "Collecting posts from r/science for June 2024\n",
      "Batch 1: 2024-06-01 to 2024-07-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-06-01 to 2024-06-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-06-01 to 2024-06-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-06-01 to 2024-06-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-06-01 to 2024-06-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-06-01 to 2024-06-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-06-01 to 2024-06-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-06-01 to 2024-06-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-06-01 to 2024-06-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-06-01 to 2024-06-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-06-01 to 2024-06-07\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-06-01 to 2024-06-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-06-01 to 2024-06-03\n",
      "Added 87 new posts (total: 1287)\n",
      " Incomplete batch (87 posts) - reached end\n",
      "Total unique posts collected: 1287\n",
      "Score range: 1 to 1\n",
      "Collected 1287 total posts from r/science for June 2024\n",
      "===Saved top posts to data/information_science_2024_06.csv===\n",
      "Collecting posts from r/science for July 2024\n",
      "Batch 1: 2024-07-01 to 2024-08-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-07-01 to 2024-07-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-07-01 to 2024-07-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-07-01 to 2024-07-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-07-01 to 2024-07-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-07-01 to 2024-07-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-07-01 to 2024-07-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-07-01 to 2024-07-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-07-01 to 2024-07-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-07-01 to 2024-07-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-07-01 to 2024-07-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-07-01 to 2024-07-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-07-01 to 2024-07-03\n",
      "Added 94 new posts (total: 1294)\n",
      " Incomplete batch (94 posts) - reached end\n",
      "Total unique posts collected: 1294\n",
      "Score range: 1 to 1\n",
      "Collected 1294 total posts from r/science for July 2024\n",
      "===Saved top posts to data/information_science_2024_07.csv===\n",
      "Collecting posts from r/science for August 2024\n",
      "Batch 1: 2024-08-01 to 2024-09-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-08-01 to 2024-08-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-08-01 to 2024-08-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-08-01 to 2024-08-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-08-01 to 2024-08-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-08-01 to 2024-08-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-08-01 to 2024-08-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-08-01 to 2024-08-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-08-01 to 2024-08-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-08-01 to 2024-08-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-08-01 to 2024-08-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-08-01 to 2024-08-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-08-01 to 2024-08-03\n",
      "Added 100 new posts (total: 1300)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1300\n",
      "Score range: 21 to 1\n",
      "Collected 1300 total posts from r/science for August 2024\n",
      "===Saved top posts to data/information_science_2024_08.csv===\n",
      "Collecting posts from r/science for September 2024\n",
      "Batch 1: 2024-09-01 to 2024-10-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-09-01 to 2024-09-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-09-01 to 2024-09-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-09-01 to 2024-09-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-09-01 to 2024-09-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-09-01 to 2024-09-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-09-01 to 2024-09-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-09-01 to 2024-09-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-09-01 to 2024-09-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-09-01 to 2024-09-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-09-01 to 2024-09-07\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-09-01 to 2024-09-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-09-01 to 2024-09-03\n",
      "Added 76 new posts (total: 1276)\n",
      " Incomplete batch (76 posts) - reached end\n",
      "Total unique posts collected: 1276\n",
      "Score range: 231 to 1\n",
      "Collected 1276 total posts from r/science for September 2024\n",
      "===Saved top posts to data/information_science_2024_09.csv===\n",
      "Collecting posts from r/science for October 2024\n",
      "Batch 1: 2024-10-01 to 2024-11-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-10-01 to 2024-10-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-10-01 to 2024-10-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-10-01 to 2024-10-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-10-01 to 2024-10-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-10-01 to 2024-10-21\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-10-01 to 2024-10-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-10-01 to 2024-10-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-10-01 to 2024-10-11\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-10-01 to 2024-10-09\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-10-01 to 2024-10-07\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-10-01 to 2024-10-04\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-10-01 to 2024-10-02\n",
      "Added 60 new posts (total: 1260)\n",
      " Incomplete batch (60 posts) - reached end\n",
      "Total unique posts collected: 1260\n",
      "Score range: 589 to 1\n",
      "Collected 1260 total posts from r/science for October 2024\n",
      "===Saved top posts to data/information_science_2024_10.csv===\n",
      "Collecting posts from r/science for November 2024\n",
      "Batch 1: 2024-11-01 to 2024-12-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-11-01 to 2024-11-28\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-11-01 to 2024-11-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-11-01 to 2024-11-23\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-11-01 to 2024-11-21\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-11-01 to 2024-11-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-11-01 to 2024-11-16\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-11-01 to 2024-11-14\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-11-01 to 2024-11-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-11-01 to 2024-11-09\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-11-01 to 2024-11-07\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-11-01 to 2024-11-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2024-11-01 to 2024-11-03\n",
      "Added 96 new posts (total: 1296)\n",
      " Incomplete batch (96 posts) - reached end\n",
      "Total unique posts collected: 1296\n",
      "Score range: 1 to 1\n",
      "Collected 1296 total posts from r/science for November 2024\n",
      "===Saved top posts to data/information_science_2024_11.csv===\n",
      "Collecting posts from r/science for December 2024\n",
      "Batch 1: 2024-12-01 to 2025-01-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2024-12-01 to 2024-12-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2024-12-01 to 2024-12-26\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2024-12-01 to 2024-12-23\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2024-12-01 to 2024-12-20\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2024-12-01 to 2024-12-18\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2024-12-01 to 2024-12-16\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2024-12-01 to 2024-12-13\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2024-12-01 to 2024-12-11\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2024-12-01 to 2024-12-08\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2024-12-01 to 2024-12-05\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2024-12-01 to 2024-12-03\n",
      "Added 79 new posts (total: 1179)\n",
      " Incomplete batch (79 posts) - reached end\n",
      "Total unique posts collected: 1179\n",
      "Score range: 1 to 1\n",
      "Collected 1179 total posts from r/science for December 2024\n",
      "===Saved top posts to data/information_science_2024_12.csv===\n",
      "Collecting posts from r/science for January 2025\n",
      "Batch 1: 2025-01-01 to 2025-02-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-01-01 to 2025-01-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-01-01 to 2025-01-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-01-01 to 2025-01-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-01-01 to 2025-01-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-01-01 to 2025-01-21\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-01-01 to 2025-01-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-01-01 to 2025-01-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-01-01 to 2025-01-14\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-01-01 to 2025-01-12\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-01-01 to 2025-01-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-01-01 to 2025-01-07\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-01-01 to 2025-01-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2025-01-01 to 2025-01-02\n",
      "Added 33 new posts (total: 1333)\n",
      " Incomplete batch (33 posts) - reached end\n",
      "Total unique posts collected: 1333\n",
      "Score range: 1 to 1\n",
      "Collected 1333 total posts from r/science for January 2025\n",
      "===Saved top posts to data/information_science_2025_01.csv===\n",
      "Collecting posts from r/science for February 2025\n",
      "Batch 1: 2025-02-01 to 2025-03-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-02-01 to 2025-02-27\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-02-01 to 2025-02-25\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-02-01 to 2025-02-22\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-02-01 to 2025-02-19\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-02-01 to 2025-02-17\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-02-01 to 2025-02-14\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-02-01 to 2025-02-12\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-02-01 to 2025-02-10\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-02-01 to 2025-02-07\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-02-01 to 2025-02-05\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-02-01 to 2025-02-03\n",
      "Added 98 new posts (total: 1198)\n",
      " Incomplete batch (98 posts) - reached end\n",
      "Total unique posts collected: 1198\n",
      "Score range: 27 to 0\n",
      "Collected 1198 total posts from r/science for February 2025\n",
      "===Saved top posts to data/information_science_2025_02.csv===\n",
      "Collecting posts from r/science for March 2025\n",
      "Batch 1: 2025-03-01 to 2025-04-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-03-01 to 2025-03-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-03-01 to 2025-03-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-03-01 to 2025-03-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-03-01 to 2025-03-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-03-01 to 2025-03-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-03-01 to 2025-03-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-03-01 to 2025-03-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-03-01 to 2025-03-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-03-01 to 2025-03-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-03-01 to 2025-03-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-03-01 to 2025-03-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-03-01 to 2025-03-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2025-03-01 to 2025-03-02\n",
      "Added 37 new posts (total: 1337)\n",
      " Incomplete batch (37 posts) - reached end\n",
      "Total unique posts collected: 1337\n",
      "Score range: 9 to 1\n",
      "Collected 1337 total posts from r/science for March 2025\n",
      "===Saved top posts to data/information_science_2025_03.csv===\n",
      "Collecting posts from r/science for April 2025\n",
      "Batch 1: 2025-04-01 to 2025-05-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-04-01 to 2025-04-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-04-01 to 2025-04-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-04-01 to 2025-04-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-04-01 to 2025-04-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-04-01 to 2025-04-19\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-04-01 to 2025-04-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-04-01 to 2025-04-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-04-01 to 2025-04-12\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-04-01 to 2025-04-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-04-01 to 2025-04-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-04-01 to 2025-04-05\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-04-01 to 2025-04-03\n",
      "Added 100 new posts (total: 1300)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1300\n",
      "Score range: 1 to 1\n",
      "Collected 1300 total posts from r/science for April 2025\n",
      "===Saved top posts to data/information_science_2025_04.csv===\n",
      "Collecting posts from r/science for May 2025\n",
      "Batch 1: 2025-05-01 to 2025-06-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-05-01 to 2025-05-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-05-01 to 2025-05-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-05-01 to 2025-05-24\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-05-01 to 2025-05-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-05-01 to 2025-05-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-05-01 to 2025-05-17\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-05-01 to 2025-05-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-05-01 to 2025-05-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-05-01 to 2025-05-10\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-05-01 to 2025-05-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-05-01 to 2025-05-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-05-01 to 2025-05-03\n",
      "Added 100 new posts (total: 1300)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1300\n",
      "Score range: 661 to 1\n",
      "Collected 1300 total posts from r/science for May 2025\n",
      "===Saved top posts to data/information_science_2025_05.csv===\n",
      "Collecting posts from r/science for June 2025\n",
      "Batch 1: 2025-06-01 to 2025-07-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-06-01 to 2025-06-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-06-01 to 2025-06-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-06-01 to 2025-06-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-06-01 to 2025-06-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-06-01 to 2025-06-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-06-01 to 2025-06-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-06-01 to 2025-06-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-06-01 to 2025-06-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-06-01 to 2025-06-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-06-01 to 2025-06-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-06-01 to 2025-06-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-06-01 to 2025-06-05\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2025-06-01 to 2025-06-03\n",
      "Added 100 new posts (total: 1400)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1400\n",
      "Score range: 1 to 1\n",
      "Collected 1400 total posts from r/science for June 2025\n",
      "===Saved top posts to data/information_science_2025_06.csv===\n",
      "Collecting posts from r/science for July 2025\n",
      "Batch 1: 2025-07-01 to 2025-08-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-07-01 to 2025-07-30\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-07-01 to 2025-07-28\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-07-01 to 2025-07-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-07-01 to 2025-07-23\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-07-01 to 2025-07-21\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-07-01 to 2025-07-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-07-01 to 2025-07-16\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-07-01 to 2025-07-14\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-07-01 to 2025-07-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-07-01 to 2025-07-09\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-07-01 to 2025-07-07\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-07-01 to 2025-07-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Batch 14: 2025-07-01 to 2025-07-02\n",
      "Added 64 new posts (total: 1364)\n",
      " Incomplete batch (64 posts) - reached end\n",
      "Total unique posts collected: 1364\n",
      "Score range: 480 to 1\n",
      "Collected 1364 total posts from r/science for July 2025\n",
      "===Saved top posts to data/information_science_2025_07.csv===\n",
      "Collecting posts from r/science for August 2025\n",
      "Batch 1: 2025-08-01 to 2025-09-01\n",
      "Added 100 new posts (total: 100)\n",
      "Batch 2: 2025-08-01 to 2025-08-29\n",
      "Added 100 new posts (total: 200)\n",
      "Batch 3: 2025-08-01 to 2025-08-27\n",
      "Added 100 new posts (total: 300)\n",
      "Batch 4: 2025-08-01 to 2025-08-25\n",
      "Added 100 new posts (total: 400)\n",
      "Batch 5: 2025-08-01 to 2025-08-22\n",
      "Added 100 new posts (total: 500)\n",
      "Batch 6: 2025-08-01 to 2025-08-20\n",
      "Added 100 new posts (total: 600)\n",
      "Batch 7: 2025-08-01 to 2025-08-18\n",
      "Added 100 new posts (total: 700)\n",
      "Batch 8: 2025-08-01 to 2025-08-15\n",
      "Added 100 new posts (total: 800)\n",
      "Batch 9: 2025-08-01 to 2025-08-13\n",
      "Added 100 new posts (total: 900)\n",
      "Batch 10: 2025-08-01 to 2025-08-11\n",
      "Added 100 new posts (total: 1000)\n",
      "Batch 11: 2025-08-01 to 2025-08-08\n",
      "Added 100 new posts (total: 1100)\n",
      "Batch 12: 2025-08-01 to 2025-08-06\n",
      "Added 100 new posts (total: 1200)\n",
      "Batch 13: 2025-08-01 to 2025-08-04\n",
      "Added 100 new posts (total: 1300)\n",
      "Reached date boundary\n",
      "Total unique posts collected: 1300\n",
      "Score range: 1 to 1\n",
      "Collected 1300 total posts from r/science for August 2025\n",
      "===Saved top posts to data/information_science_2025_08.csv===\n"
     ]
    }
   ],
   "source": [
    "# info_subreddits = ['science', 'AmItheAsshole', 'changemyview']\n",
    "\n",
    "info_subreddits = ['science']\n",
    "\n",
    "for subreddit in info_subreddits:\n",
    "    for year in [2023, 2024, 2025]:\n",
    "        start_month = 8 if year == 2023 else 1\n",
    "        end_month = 8 if year == 2025 else 12\n",
    "        \n",
    "        for month in range(start_month, end_month + 1):\n",
    "            top_posts = get_top_posts_arctic_shift(subreddit, month, year, n=50)\n",
    "            top_posts.to_csv(f\"data/information/{subreddit}_{year}_{month:02d}.csv\", index=False)\n",
    "            print(f\"===Saved top posts to data/information_{subreddit}_{year}_{month:02d}.csv===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] science: expected 25 files, found 26\n",
      "[warn] AmItheAsshole: expected 25 files, found 26\n",
      "[warn] changemyview: expected 25 files, found 26\n",
      "science: (704, 13)\n",
      "AmItheAsshole: (1250, 13)\n",
      "changemyview: (1250, 13)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"data/information\"\n",
    "SUBS = ['science', 'AmItheAsshole', 'changemyview']\n",
    "\n",
    "def load_combine_subreddit(subreddit: str) -> pd.DataFrame:\n",
    "    pattern = os.path.join(BASE_DIR, f\"{subreddit}_*.csv\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if len(files) != 25:\n",
    "        print(f\"[warn] {subreddit}: expected 25 files, found {len(files)}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "            if not df.empty:\n",
    "                frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {fp}: {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Normalize types\n",
    "    if \"created_datetime\" in out.columns:\n",
    "        out[\"created_datetime\"] = pd.to_datetime(out[\"created_datetime\"], errors=\"coerce\")\n",
    "    if \"created_utc\" in out.columns:\n",
    "        out[\"created_utc\"] = pd.to_numeric(out[\"created_utc\"], errors=\"coerce\")\n",
    "\n",
    "    # Deduplicate (prefer stable post_id if present)\n",
    "    if \"post_id\" in out.columns:\n",
    "        out = out.drop_duplicates(subset=\"post_id\")\n",
    "    elif {\"permalink\", \"title\", \"text\"}.issubset(out.columns):\n",
    "        out = out.drop_duplicates(subset=[\"permalink\", \"title\", \"text\"])\n",
    "    else:\n",
    "        out = out.drop_duplicates()\n",
    "\n",
    "    # Sort by time if available\n",
    "    if \"created_utc\" in out.columns:\n",
    "        out = out.sort_values(\"created_utc\")\n",
    "    elif \"created_datetime\" in out.columns:\n",
    "        out = out.sort_values(\"created_datetime\")\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "df_science        = load_combine_subreddit(\"science\")\n",
    "df_amitheasshole  = load_combine_subreddit(\"AmItheAsshole\")\n",
    "df_changemyview   = load_combine_subreddit(\"changemyview\")\n",
    "\n",
    "# print(\"science:\", df_science.shape)\n",
    "# print(\"AmItheAsshole:\", df_amitheasshole.shape)\n",
    "# print(\"changemyview:\", df_changemyview.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top by performance  science\n",
      "post_id                                                                                                                                                                                                                                                                       title  score  num_comments  performance_score    created_datetime\n",
      "16hetvy Highly Sensitive Persons (HSPs) are more sensitive than the average person to various stimuli (e.g., light, noise, temperature, touch). HSPs have stronger than average reactions to negative events but not positive ones, according to research published in August 2023.   1232           285           1.000000 2023-09-13 00:50:54\n",
      "1kjqj89                                                                                                                                              RETRACTION: A Tunguska sized airburst destroyed Tall el-Hammam a Middle Bronze Age city in the Jordan Valley near the Dead Sea    661            48           0.352474 2025-05-10 21:23:50\n",
      "1fzp6hu                                                                  The Nobel Prize in Chemistry 2024: Awarded with one half to David Baker for \"computational protein design\" and the other half jointly to Demis Hassabis and John Jumper for \"protein structure prediction\"    589            41           0.310972 2024-10-09 06:30:15\n",
      "1fyxla3                                                                           The Nobel Prize in Physics 2024: Awarded jointly to John Hopfield and Geoffrey Hinton for \"foundational discoveries and inventions that enable machine learning with artificial neural networks.    427            72           0.299611 2024-10-08 06:34:53\n",
      "1mc1lni                                                                                                                                                                                                 RETRACTED: A Bacterium That Can Grow by Using Arsenic Instead of Phosphorus    480            29           0.245682 2025-07-28 23:02:47\n",
      "\n",
      "Top by performance  AmItheAsshole\n",
      "post_id                                                                                                                                             title  score  num_comments  performance_score    created_datetime\n",
      "161filb AITA Job pays $100k, wife complains I need a different job because I dont make enough money to support our family I told her thats ridiculous  17200          7795           0.769050 2023-08-25 18:59:52\n",
      "1n2odkk                                                                                    AITA - Do not want a service dog to participate in my wedding.   9668          8795           0.677790 2025-08-28 16:09:43\n",
      "1gg08k9                                                         AITA for not being friendly with my partners daughters now that they've \"warmed up\" to me  23739          3090           0.630127 2024-10-30 18:14:07\n",
      "177c2tj                                                                                   AITA for refusing to let my wife name our kid something stupid?  14671          6117           0.623911 2023-10-13 18:39:05\n",
      "1elut6n                                                                       AITA for refusing my girlfriends request of peeing sitting down in our home   8411          7656           0.588312 2024-08-06 17:10:33\n",
      "\n",
      "Top by performance  changemyview\n",
      "post_id                                                                                                                title  score  num_comments  performance_score    created_datetime\n",
      "1iht2i4           CMV: The new DNC Vice Chair David Hogg exemplifies exactly why the Democratic Party lost the 2024 election   8203          2729           0.766181 2025-02-04 15:29:20\n",
      "1mblqmi  CMV: If you're a centrist, and a leftist being mean to you pushes you to the right, you were always a right winger.   6132          3336           0.725734 2025-07-28 11:59:18\n",
      "1fi41o4 CMV: Voting for Donald Trump in the 2024 election means you're either ill informed or actively opposed to democracy.   4385          4101           0.721968 2024-09-16 08:02:20\n",
      "1ing2ad                            CMV: Elon Musk walks around with his son on his shoulders to deter assassination attempts   9321          1756           0.710583 2025-02-11 20:04:01\n",
      "1iu4qpq                                                                          CMV: the Democratic Party isnt up for this   8114          2293           0.708997 2025-02-20 11:57:44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def minmax_norm(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    smin, smax = s.min(), s.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series(0.5, index=s.index)  # flat/invalid -> neutral 0.5\n",
    "    return (s - smin) / (smax - smin)\n",
    "\n",
    "def add_performance_score(df: pd.DataFrame,\n",
    "                          score_col=\"score\",\n",
    "                          comments_col=\"num_comments\",\n",
    "                          w_score=0.5,\n",
    "                          w_comments=0.5) -> pd.DataFrame:\n",
    "    if score_col not in df.columns or comments_col not in df.columns:\n",
    "        missing = [c for c in (score_col, comments_col) if c not in df.columns]\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "    out = df.copy()\n",
    "    out[\"score_norm\"] = minmax_norm(out[score_col])\n",
    "    out[\"comments_norm\"] = minmax_norm(out[comments_col])\n",
    "    out[\"performance_score\"] = w_score * out[\"score_norm\"] + w_comments * out[\"comments_norm\"]\n",
    "    return out\n",
    "\n",
    "# Apply to each subreddit DF\n",
    "df_science       = add_performance_score(df_science)\n",
    "df_amitheasshole = add_performance_score(df_amitheasshole)\n",
    "df_changemyview  = add_performance_score(df_changemyview)\n",
    "\n",
    "# (optional) peek at the top posts by performance\n",
    "for name, df in {\n",
    "    \"science\": df_science,\n",
    "    \"AmItheAsshole\": df_amitheasshole,\n",
    "    \"changemyview\": df_changemyview\n",
    "}.items():\n",
    "    print(f\"\\nTop by performance  {name}\")\n",
    "    cols = [c for c in [\"post_id\",\"title\",\"score\",\"num_comments\",\"performance_score\",\"created_datetime\"] if c in df.columns]\n",
    "    print(df.sort_values(\"performance_score\", ascending=False)[cols].head(5).to_string(index=False))\n",
    "\n",
    "# (optional) save back out\n",
    "# df_science.to_csv(\"data/information/science_all_with_perf.csv\", index=False)\n",
    "# df_amitheasshole.to_csv(\"data/information/AmItheAsshole_all_with_perf.csv\", index=False)\n",
    "# df_changemyview.to_csv(\"data/information/changemyview_all_with_perf.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- basic cleaner (lowercase, drop URLs, remove punctuation, normalize spaces) ---\n",
    "url_re   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "punct_re = re.compile(r\"[^a-zA-Z0-9\\s]\")  # keep letters, numbers, spaces\n",
    "space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def basic_clean_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str).str.lower()\n",
    "    s = s.str.replace(url_re, \"\", regex=True)\n",
    "    s = s.str.replace(punct_re, \" \", regex=True)\n",
    "    s = s.str.replace(space_re, \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "def clean_title_text(df: pd.DataFrame, title_col=\"title\", text_col=\"text\", overwrite=False) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if title_col in out.columns:\n",
    "        cleaned = basic_clean_series(out[title_col])\n",
    "        if overwrite:\n",
    "            out[title_col] = cleaned\n",
    "        else:\n",
    "            out[\"title_clean\"] = cleaned\n",
    "    if text_col in out.columns:\n",
    "        cleaned = basic_clean_series(out[text_col])\n",
    "        if overwrite:\n",
    "            out[text_col] = cleaned\n",
    "        else:\n",
    "            out[\"text_clean\"] = cleaned\n",
    "    return out\n",
    "\n",
    "df_science        = clean_title_text(df_science)\n",
    "df_amitheasshole  = clean_title_text(df_amitheasshole)\n",
    "df_changemyview   = clean_title_text(df_changemyview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- basic cleaner (lowercase, drop URLs, remove punctuation, normalize spaces) ---\n",
    "url_re   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "punct_re = re.compile(r\"[^a-zA-Z0-9\\s]\")  # keep letters, numbers, spaces\n",
    "space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def basic_clean_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str).str.lower()\n",
    "    s = s.str.replace(url_re, \"\", regex=True)\n",
    "    s = s.str.replace(punct_re, \" \", regex=True)\n",
    "    s = s.str.replace(space_re, \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "def clean_title_text(df: pd.DataFrame, title_col=\"title\", text_col=\"text\", overwrite=False) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if title_col in out.columns:\n",
    "        cleaned = basic_clean_series(out[title_col])\n",
    "        if overwrite:\n",
    "            out[title_col] = cleaned\n",
    "        else:\n",
    "            out[\"title_clean\"] = cleaned\n",
    "    if text_col in out.columns:\n",
    "        cleaned = basic_clean_series(out[text_col])\n",
    "        if overwrite:\n",
    "            out[text_col] = cleaned\n",
    "        else:\n",
    "            out[\"text_clean\"] = cleaned\n",
    "    return out\n",
    "\n",
    "# Apply to each subreddit DataFrame (keeping originals)\n",
    "df_science        = clean_title_text(df_science)\n",
    "df_amitheasshole  = clean_title_text(df_amitheasshole)\n",
    "df_changemyview   = clean_title_text(df_changemyview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_1182/3636098484.py:49: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"is_success\"] = df.groupby([\"year\",\"month\"], group_keys=False).apply(label_success)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: n=833, positives=167 (20.0%), negatives=666 (80.0%)\n",
      "Validation: n=417, positives=83 (19.9%), negatives=334 (80.1%)\n",
      "X_train shape: (833, 1) | X_val shape: (417, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- my basic settings ----------\n",
    "SUCCESS_QUANTILE = 0.80     # top 20% within (year, month) = success\n",
    "TIME_BASED_SPLIT = False     # set True to validate on future months instead of random split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = df_changemyview.copy()\n",
    "if \"created_datetime\" in df.columns:\n",
    "    df[\"created_datetime\"] = pd.to_datetime(df[\"created_datetime\"], errors=\"coerce\")\n",
    "elif \"created_utc\" in df.columns:\n",
    "    df[\"created_datetime\"] = pd.to_datetime(pd.to_numeric(df[\"created_utc\"], errors=\"coerce\"), unit=\"s\", errors=\"coerce\")\n",
    "else:\n",
    "    df[\"created_datetime\"] = pd.to_datetime(dict(year=df[\"year\"], month=df[\"month\"], day=1), errors=\"coerce\")\n",
    "\n",
    "def minmax_norm(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    smin, smax = s.min(), s.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series(0.5, index=s.index)\n",
    "    return (s - smin) / (smax - smin)\n",
    "\n",
    "if \"performance_score\" not in df.columns:\n",
    "    if {\"score\",\"num_comments\"}.issubset(df.columns):\n",
    "        df[\"score_norm\"] = minmax_norm(df[\"score\"])\n",
    "        df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"])\n",
    "        df[\"performance_score\"] = 0.5*df[\"score_norm\"] + 0.5*df[\"comments_norm\"]\n",
    "    else:\n",
    "        raise KeyError(\"Need 'performance_score' or 'score' & 'num_comments' to compute it.\")\n",
    "\n",
    "def label_success(group: pd.DataFrame) -> pd.Series:\n",
    "    ps = group[\"performance_score\"]\n",
    "    if ps.nunique() <= 1:\n",
    "        # if flat month, call no one a success\n",
    "        thresh = np.inf\n",
    "    else:\n",
    "        thresh = ps.quantile(SUCCESS_QUANTILE)\n",
    "    return (ps >= thresh).astype(int)\n",
    "\n",
    "if {\"year\",\"month\"}.issubset(df.columns):\n",
    "    df[\"is_success\"] = df.groupby([\"year\",\"month\"], group_keys=False).apply(label_success)\n",
    "else:\n",
    "    thresh = df[\"performance_score\"].quantile(SUCCESS_QUANTILE)\n",
    "    df[\"is_success\"] = (df[\"performance_score\"] >= thresh).astype(int)\n",
    "\n",
    "title_col = \"title_clean\" if \"title_clean\" in df.columns else \"title\"\n",
    "text_col  = \"text_clean\"  if \"text_clean\"  in df.columns else \"text\"\n",
    "for c in (title_col, text_col):\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\"\n",
    "\n",
    "df[\"text_full_clean\"] = (\n",
    "    df[title_col].fillna(\"\").astype(str).str.strip() + \" \" +\n",
    "    df[text_col].fillna(\"\").astype(str).str.strip()\n",
    ").str.strip()\n",
    "\n",
    "df = df[df[\"text_full_clean\"].str.len() > 0].copy()\n",
    "\n",
    "FEATURE_COLS = [\"text_full_clean\"]  # add more later (title length, time of day, season, text body length, LIQC and sentiment of texts in posts)\n",
    "TARGET_COL = \"is_success\"\n",
    "\n",
    "if TIME_BASED_SPLIT:\n",
    "    df = df.sort_values(\"created_datetime\")\n",
    "    cutoff_idx = int(len(df) * (2/3))\n",
    "    train_df = df.iloc[:cutoff_idx].copy()\n",
    "    val_df   = df.iloc[cutoff_idx:].copy()\n",
    "else:\n",
    "    # Stratified random split by label\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=1/3, random_state=RANDOM_STATE, stratify=df[TARGET_COL]\n",
    "    )\n",
    "\n",
    "def describe_split(name, dframe):\n",
    "    n = len(dframe)\n",
    "    pos = int(dframe[TARGET_COL].sum())\n",
    "    neg = n - pos\n",
    "    print(f\"{name}: n={n}, positives={pos} ({pos/n:.1%}), negatives={neg} ({neg/n:.1%})\")\n",
    "\n",
    "describe_split(\"Train\", train_df)\n",
    "describe_split(\"Validation\", val_df)\n",
    "\n",
    "train_df.to_csv(\"data/information/changemyview_train.csv\", index=False)\n",
    "val_df.to_csv(\"data/information/changemyview_val.csv\", index=False)\n",
    "\n",
    "X_train, y_train = train_df[FEATURE_COLS], train_df[TARGET_COL]\n",
    "X_val,   y_val   = val_df[FEATURE_COLS],   val_df[TARGET_COL]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"| X_val shape:\", X_val.shape)\n",
    "\n",
    "\n",
    "# Train: n=833, positives=167 (20.0%), negatives=666 (80.0%)\n",
    "# Validation: n=417, positives=83 (19.9%), negatives=334 (80.1%)\n",
    "# X_train shape: (833, 1) | X_val shape: (417, 1)\n",
    "\n",
    "# WHAT ABOVE MEANS: \n",
    "# - we are training based on data from  r/changemyview\n",
    "# - training data has 833 rows, 167 rows labeled as 'successful' posts, 666 labeled as 'unsuccessful' posts\n",
    "# - validation row has 417 rows, 833 rows labeled as 'successful' posts, 334 labeled as 'unsuccessful' posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.715  Precision: 0.324  Recall: 0.398\n",
      "\n",
      "Top tokens  predict SUCCESS (+):\n",
      "men                        1.901\n",
      "left                       0.935\n",
      "israel                     0.922\n",
      "white                      0.876\n",
      "the left                   0.845\n",
      "democrats                  0.830\n",
      "women                      0.737\n",
      "election                   0.719\n",
      "attack                     0.686\n",
      "of                         0.658\n",
      "his                        0.655\n",
      "test                       0.651\n",
      "saying                     0.639\n",
      "young                      0.632\n",
      "would                      0.620\n",
      "\n",
      "Top tokens  predict NON-SUCCESS ():\n",
      "we                        -0.911\n",
      "want                      -0.793\n",
      "into                      -0.747\n",
      "the                       -0.707\n",
      "they                      -0.690\n",
      "re                        -0.661\n",
      "want to                   -0.611\n",
      "let                       -0.549\n",
      "your                      -0.547\n",
      "other                     -0.534\n",
      "food                      -0.529\n",
      "it                        -0.507\n",
      "police                    -0.504\n",
      "christian                 -0.501\n",
      "to                        -0.497\n"
     ]
    }
   ],
   "source": [
    "# ===== Train & minimal eval =====\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Core metrics on validation\n",
    "y_pred = pipe.predict(X_val)\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec = precision_score(y_val, y_pred, zero_division=0)  # positive class = 1\n",
    "rec  = recall_score(y_val, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}  Precision: {prec:.3f}  Recall: {rec:.3f}\")\n",
    "\n",
    "# ===== Top tokens (from TF-IDF) that push toward success/non-success =====\n",
    "prep = pipe.named_steps[\"prep\"]\n",
    "clf  = pipe.named_steps[\"clf\"]\n",
    "\n",
    "# TF-IDF branch and token names\n",
    "tfidf = prep.named_transformers_[\"tfidf\"]\n",
    "token_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Coefficients come out in the same order as the ColumnTransformer output:\n",
    "# [ TF-IDF features , numeric features , categorical features ]\n",
    "coef_all = clf.coef_.ravel()\n",
    "k = len(token_names)            # number of TF-IDF features\n",
    "token_coefs = coef_all[:k]      # slice just the TF-IDF part\n",
    "\n",
    "top_n = 15\n",
    "top_pos_idx = np.argsort(token_coefs)[-top_n:][::-1]\n",
    "top_neg_idx = np.argsort(token_coefs)[:top_n]\n",
    "\n",
    "print(\"\\nTop tokens  predict SUCCESS (+):\")\n",
    "for i in top_pos_idx:\n",
    "    print(f\"{token_names[i]:<25} {token_coefs[i]: .3f}\")\n",
    "\n",
    "print(\"\\nTop tokens  predict NON-SUCCESS ():\")\n",
    "for i in top_neg_idx:\n",
    "    print(f\"{token_names[i]:<25} {token_coefs[i]: .3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
>>>>>>> origin/main
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Note: 50% of the performace of a post is based off of upvote score, 50% of the performance of a post is based on number of comments for each post"
=======
    "#### Note: Post 'success' performance is 50% weighted by upvote score, 50% weighted by number of comments for each post"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/Science  2025-10\n",
      "  saved 50  ./raw_data/Science/2025-10_top50.csv\n",
      "r/Science  2025-09\n",
      "  saved 50  ./raw_data/Science/2025-09_top50.csv\n",
      "r/Science  2025-08\n",
      "  saved 50  ./raw_data/Science/2025-08_top50.csv\n",
      "r/Science  2025-07\n",
      "  saved 50  ./raw_data/Science/2025-07_top50.csv\n",
      "r/Science  2025-06\n",
      "  saved 50  ./raw_data/Science/2025-06_top50.csv\n",
      "r/Science  2025-05\n",
      "  saved 50  ./raw_data/Science/2025-05_top50.csv\n",
      "r/Science  2025-04\n",
      "  saved 50  ./raw_data/Science/2025-04_top50.csv\n",
      "r/Science  2025-03\n",
      "  saved 50  ./raw_data/Science/2025-03_top50.csv\n",
      "r/Science  2025-02\n",
      "  saved 50  ./raw_data/Science/2025-02_top50.csv\n",
      "r/Science  2025-01\n",
      "  saved 50  ./raw_data/Science/2025-01_top50.csv\n",
      "r/Science  2024-12\n",
      "  saved 50  ./raw_data/Science/2024-12_top50.csv\n",
      "r/Science  2024-11\n",
      "  saved 50  ./raw_data/Science/2024-11_top50.csv\n",
      "r/Science  2024-10\n",
      "  saved 50  ./raw_data/Science/2024-10_top50.csv\n",
      "r/Science  2024-09\n",
      "  saved 50  ./raw_data/Science/2024-09_top50.csv\n",
      "r/Science  2024-08\n",
      "  saved 50  ./raw_data/Science/2024-08_top50.csv\n",
      "r/Science  2024-07\n",
      "  saved 50  ./raw_data/Science/2024-07_top50.csv\n",
      "r/Science  2024-06\n",
      "  saved 50  ./raw_data/Science/2024-06_top50.csv\n",
      "r/Science  2024-05\n",
      "  saved 50  ./raw_data/Science/2024-05_top50.csv\n",
      "r/Science  2024-04\n",
      "  saved 50  ./raw_data/Science/2024-04_top50.csv\n",
      "r/Science  2024-03\n",
      "  saved 50  ./raw_data/Science/2024-03_top50.csv\n",
      "r/Science  2024-02\n",
      "  saved 50  ./raw_data/Science/2024-02_top50.csv\n",
      "r/Science  2024-01\n",
      "  saved 50  ./raw_data/Science/2024-01_top50.csv\n",
      "r/Science  2023-12\n",
      "  saved 50  ./raw_data/Science/2023-12_top50.csv\n",
      "r/Science  2023-11\n",
      "  saved 50  ./raw_data/Science/2023-11_top50.csv\n",
      "r/changemyview  2025-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  saved 50  ./raw_data/changemyview/2025-10_top50.csv\n",
      "r/changemyview  2025-09\n",
      "  saved 50  ./raw_data/changemyview/2025-09_top50.csv\n",
      "r/changemyview  2025-08\n",
      "  saved 50  ./raw_data/changemyview/2025-08_top50.csv\n",
      "r/changemyview  2025-07\n",
      "  saved 50  ./raw_data/changemyview/2025-07_top50.csv\n",
      "r/changemyview  2025-06\n",
      "  saved 50  ./raw_data/changemyview/2025-06_top50.csv\n",
      "r/changemyview  2025-05\n",
      "  saved 50  ./raw_data/changemyview/2025-05_top50.csv\n",
      "r/changemyview  2025-04\n",
      "  saved 50  ./raw_data/changemyview/2025-04_top50.csv\n",
      "r/changemyview  2025-03\n",
      "  saved 50  ./raw_data/changemyview/2025-03_top50.csv\n",
      "r/changemyview  2025-02\n",
      "  saved 50  ./raw_data/changemyview/2025-02_top50.csv\n",
      "r/changemyview  2025-01\n",
      "  saved 50  ./raw_data/changemyview/2025-01_top50.csv\n",
      "r/changemyview  2024-12\n",
      "  saved 50  ./raw_data/changemyview/2024-12_top50.csv\n",
      "r/changemyview  2024-11\n",
      "  saved 50  ./raw_data/changemyview/2024-11_top50.csv\n",
      "r/changemyview  2024-10\n",
      "  saved 50  ./raw_data/changemyview/2024-10_top50.csv\n",
      "r/changemyview  2024-09\n",
      "  saved 50  ./raw_data/changemyview/2024-09_top50.csv\n",
      "r/changemyview  2024-08\n",
      "  saved 50  ./raw_data/changemyview/2024-08_top50.csv\n",
      "r/changemyview  2024-07\n",
      "  saved 50  ./raw_data/changemyview/2024-07_top50.csv\n",
      "r/changemyview  2024-06\n",
      "  saved 50  ./raw_data/changemyview/2024-06_top50.csv\n",
      "r/changemyview  2024-05\n",
      "  saved 50  ./raw_data/changemyview/2024-05_top50.csv\n",
      "r/changemyview  2024-04\n",
      "  saved 50  ./raw_data/changemyview/2024-04_top50.csv\n",
      "r/changemyview  2024-03\n",
      "  saved 50  ./raw_data/changemyview/2024-03_top50.csv\n",
      "r/changemyview  2024-02\n",
      "  saved 50  ./raw_data/changemyview/2024-02_top50.csv\n",
      "r/changemyview  2024-01\n",
      "  saved 50  ./raw_data/changemyview/2024-01_top50.csv\n",
      "r/changemyview  2023-12\n",
      "  saved 50  ./raw_data/changemyview/2023-12_top50.csv\n",
      "r/changemyview  2023-11\n",
      "  saved 50  ./raw_data/changemyview/2023-11_top50.csv\n",
      "r/AmItheAsshole  2025-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_48563/542338546.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  anchor = datetime.utcnow().replace(day=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  saved 50  ./raw_data/AmItheAsshole/2025-10_top50.csv\n",
      "r/AmItheAsshole  2025-09\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-09_top50.csv\n",
      "r/AmItheAsshole  2025-08\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-08_top50.csv\n",
      "r/AmItheAsshole  2025-07\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-07_top50.csv\n",
      "r/AmItheAsshole  2025-06\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-06_top50.csv\n",
      "r/AmItheAsshole  2025-05\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-05_top50.csv\n",
      "r/AmItheAsshole  2025-04\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-04_top50.csv\n",
      "r/AmItheAsshole  2025-03\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-03_top50.csv\n",
      "r/AmItheAsshole  2025-02\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-02_top50.csv\n",
      "r/AmItheAsshole  2025-01\n",
      "  saved 50  ./raw_data/AmItheAsshole/2025-01_top50.csv\n",
      "r/AmItheAsshole  2024-12\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-12_top50.csv\n",
      "r/AmItheAsshole  2024-11\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-11_top50.csv\n",
      "r/AmItheAsshole  2024-10\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-10_top50.csv\n",
      "r/AmItheAsshole  2024-09\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-09_top50.csv\n",
      "r/AmItheAsshole  2024-08\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-08_top50.csv\n",
      "r/AmItheAsshole  2024-07\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-07_top50.csv\n",
      "r/AmItheAsshole  2024-06\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-06_top50.csv\n",
      "r/AmItheAsshole  2024-05\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-05_top50.csv\n",
      "r/AmItheAsshole  2024-04\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-04_top50.csv\n",
      "r/AmItheAsshole  2024-03\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-03_top50.csv\n",
      "r/AmItheAsshole  2024-02\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-02_top50.csv\n",
      "r/AmItheAsshole  2024-01\n",
      "  saved 50  ./raw_data/AmItheAsshole/2024-01_top50.csv\n",
      "r/AmItheAsshole  2023-12\n",
      "  saved 50  ./raw_data/AmItheAsshole/2023-12_top50.csv\n",
      "r/AmItheAsshole  2023-11\n",
      "  saved 50  ./raw_data/AmItheAsshole/2023-11_top50.csv\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "def fetch_posts(subreddit, after, before, limit=100):\n",
    "    params = {\n",
    "        \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "        \"after\": after, \"before\": before,\n",
    "        \"limit\": limit, \"sort\": \"desc\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(API_URL, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch err] {subreddit} {after}{before}: {e}\")\n",
    "        return []\n",
    "\n",
    "def minmax_norm(series):\n",
    "    smin, smax = series.min(), series.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series([0.5] * len(series), index=series.index)  # flat month  neutral 0.5\n",
    "    return (series - smin) / (smax - smin)\n",
    "\n",
    "def collect_month(subreddit, year, month):\n",
    "    # month bounds\n",
    "    start = datetime(year, month, 1)\n",
    "    end   = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "    after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    seen, rows, cursor = set(), [], before\n",
    "    while True:\n",
    "        batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "        if not batch:\n",
    "            break\n",
    "        for p in batch:\n",
    "            pid = p.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen.add(pid)\n",
    "                rows.append(p)\n",
    "        oldest_ts = min(p.get(\"created_utc\", 0) for p in batch)\n",
    "        cursor = datetime.fromtimestamp(oldest_ts).strftime(\"%Y-%m-%d\")\n",
    "        if cursor <= after:\n",
    "            break\n",
    "        time.sleep(1)  # be nice to the API\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "    # 50/50 popularity = normalized score (50%) + normalized comments (50%)\n",
    "    if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "        df[\"score_norm\"] = minmax_norm(df[\"score\"].astype(\"float\"))\n",
    "        df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].astype(\"float\"))\n",
    "        df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "        df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "    else:\n",
    "        # fallback: if fields missing, keep top 50 by score\n",
    "        df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def last_n_months(n=24):\n",
    "    # step back month-by-month from the first of the current month\n",
    "    anchor = datetime.utcnow().replace(day=1)\n",
    "    y, m = anchor.year, anchor.month\n",
    "    for _ in range(n):\n",
    "        yield y, m\n",
    "        m -= 1\n",
    "        if m == 0:\n",
    "            m, y = 12, y - 1\n",
    "\n",
    "for sub in subreddit_communities:\n",
    "    sub_name = sub.replace(\"r/\", \"\")\n",
    "    out_dir = os.path.join(data_dir, sub_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for y, m in last_n_months(24):\n",
    "        print(f\"{sub}  {y}-{m:02d}\")\n",
    "        dfm = collect_month(sub, y, m)\n",
    "        if dfm.empty:\n",
    "            print(\"  (no data)\")\n",
    "            continue\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "        dfm[[\n",
    "            \"id\",\"title\",\"selftext\",\"author\",\"score\",\"num_comments\",\"upvote_ratio\",\n",
    "            \"popularity\",\"created_utc\",\"permalink\",\"year\",\"month\"\n",
    "        ]].to_csv(out_path, index=False)\n",
    "        print(f\"  saved {len(dfm)}  {out_path}\")"
=======
    "# def fetch_posts(subreddit, after, before, limit=100):\n",
    "#     params = {\n",
    "#         \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "#         \"after\": after, \"before\": before,\n",
    "#         \"limit\": limit, \"sort\": \"desc\"\n",
    "#     }\n",
    "#     try:\n",
    "#         r = requests.get(API_URL, params=params, timeout=30)\n",
    "#         r.raise_for_status()\n",
    "#         return r.json().get(\"data\", [])\n",
    "#     except Exception as e:\n",
    "#         print(f\"[fetch err] {subreddit} {after}{before}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# def minmax_norm(series):\n",
    "#     smin, smax = series.min(), series.max()\n",
    "#     if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "#         return pd.Series([0.5] * len(series), index=series.index)  # flat month  neutral 0.5\n",
    "#     return (series - smin) / (smax - smin)\n",
    "\n",
    "# def collect_month(subreddit, year, month):\n",
    "#     # month bounds\n",
    "#     start = datetime(year, month, 1)\n",
    "#     end   = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "#     after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#     seen, rows, cursor = set(), [], before\n",
    "#     while True:\n",
    "#         batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "#         if not batch:\n",
    "#             break\n",
    "#         for p in batch:\n",
    "#             pid = p.get(\"id\")\n",
    "#             if pid and pid not in seen:\n",
    "#                 seen.add(pid)\n",
    "#                 rows.append(p)\n",
    "#         oldest_ts = min(p.get(\"created_utc\", 0) for p in batch)\n",
    "#         cursor = datetime.fromtimestamp(oldest_ts).strftime(\"%Y-%m-%d\")\n",
    "#         if cursor <= after:\n",
    "#             break\n",
    "#         time.sleep(1)  # be nice to the API\n",
    "\n",
    "#     if not rows:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "#     # 50/50 popularity = normalized score (50%) + normalized comments (50%)\n",
    "#     if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "#         df[\"score_norm\"] = minmax_norm(df[\"score\"].astype(\"float\"))\n",
    "#         df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].astype(\"float\"))\n",
    "#         df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "#         df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "#     else:\n",
    "#         # fallback: if fields missing, keep top 50 by score\n",
    "#         df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def last_n_months(n=24):\n",
    "#     # step back month-by-month from the first of the current month\n",
    "#     anchor = datetime.utcnow().replace(day=1)\n",
    "#     y, m = anchor.year, anchor.month\n",
    "#     for _ in range(n):\n",
    "#         yield y, m\n",
    "#         m -= 1\n",
    "#         if m == 0:\n",
    "#             m, y = 12, y - 1\n",
    "\n",
    "# for sub in subreddit_communities:\n",
    "#     sub_name = sub.replace(\"r/\", \"\")\n",
    "#     out_dir = os.path.join(data_dir, sub_name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     for y, m in last_n_months(24):\n",
    "#         print(f\"{sub}  {y}-{m:02d}\")\n",
    "#         dfm = collect_month(sub, y, m)\n",
    "#         if dfm.empty:\n",
    "#             print(\"  (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "#         dfm[[\n",
    "#             \"id\",\"title\",\"selftext\",\"author\",\"score\",\"num_comments\",\"upvote_ratio\",\n",
    "#             \"popularity\",\"created_utc\",\"permalink\",\"year\",\"month\"\n",
    "#         ]].to_csv(out_path, index=False)\n",
    "#         print(f\"  saved {len(dfm)}  {out_path}\")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/Science  2023-09\n",
      "  saved 50  ./raw_25month_range_data/Science/2023-09_top50.csv\n",
      "r/Science  2023-10\n",
      "  saved 50  ./raw_25month_range_data/Science/2023-10_top50.csv\n",
      "r/Science  2023-11\n",
      "  saved 50  ./raw_25month_range_data/Science/2023-11_top50.csv\n",
      "r/Science  2023-12\n",
      "  saved 50  ./raw_25month_range_data/Science/2023-12_top50.csv\n",
      "r/Science  2024-01\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-01_top50.csv\n",
      "r/Science  2024-02\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-02_top50.csv\n",
      "r/Science  2024-03\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-03_top50.csv\n",
      "r/Science  2024-04\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-04_top50.csv\n",
      "r/Science  2024-05\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-05_top50.csv\n",
      "r/Science  2024-06\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-06_top50.csv\n",
      "r/Science  2024-07\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-07_top50.csv\n",
      "r/Science  2024-08\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-08_top50.csv\n",
      "r/Science  2024-09\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-09_top50.csv\n",
      "r/Science  2024-10\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-10_top50.csv\n",
      "r/Science  2024-11\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-11_top50.csv\n",
      "r/Science  2024-12\n",
      "  saved 50  ./raw_25month_range_data/Science/2024-12_top50.csv\n",
      "r/Science  2025-01\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-01_top50.csv\n",
      "r/Science  2025-02\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-02_top50.csv\n",
      "r/Science  2025-03\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-03_top50.csv\n",
      "r/Science  2025-04\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-04_top50.csv\n",
      "r/Science  2025-05\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-05_top50.csv\n",
      "r/Science  2025-06\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-06_top50.csv\n",
      "r/Science  2025-07\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-07_top50.csv\n",
      "r/Science  2025-08\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-08_top50.csv\n",
      "r/Science  2025-09\n",
      "  saved 50  ./raw_25month_range_data/Science/2025-09_top50.csv\n",
      "r/changemyview  2023-09\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2023-09_top50.csv\n",
      "r/changemyview  2023-10\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2023-10_top50.csv\n",
      "r/changemyview  2023-11\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2023-11_top50.csv\n",
      "r/changemyview  2023-12\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2023-12_top50.csv\n",
      "r/changemyview  2024-01\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-01_top50.csv\n",
      "r/changemyview  2024-02\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-02_top50.csv\n",
      "r/changemyview  2024-03\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-03_top50.csv\n",
      "r/changemyview  2024-04\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-04_top50.csv\n",
      "r/changemyview  2024-05\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-05_top50.csv\n",
      "r/changemyview  2024-06\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-06_top50.csv\n",
      "r/changemyview  2024-07\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-07_top50.csv\n",
      "r/changemyview  2024-08\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-08_top50.csv\n",
      "r/changemyview  2024-09\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-09_top50.csv\n",
      "r/changemyview  2024-10\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-10_top50.csv\n",
      "r/changemyview  2024-11\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-11_top50.csv\n",
      "r/changemyview  2024-12\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2024-12_top50.csv\n",
      "r/changemyview  2025-01\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-01_top50.csv\n",
      "r/changemyview  2025-02\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-02_top50.csv\n",
      "r/changemyview  2025-03\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-03_top50.csv\n",
      "r/changemyview  2025-04\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-04_top50.csv\n",
      "r/changemyview  2025-05\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-05_top50.csv\n",
      "r/changemyview  2025-06\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-06_top50.csv\n",
      "r/changemyview  2025-07\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-07_top50.csv\n",
      "r/changemyview  2025-08\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-08_top50.csv\n",
      "r/changemyview  2025-09\n",
      "  saved 50  ./raw_25month_range_data/changemyview/2025-09_top50.csv\n",
      "r/AmItheAsshole  2023-09\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2023-09_top50.csv\n",
      "r/AmItheAsshole  2023-10\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2023-10_top50.csv\n",
      "r/AmItheAsshole  2023-11\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2023-11_top50.csv\n",
      "r/AmItheAsshole  2023-12\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2023-12_top50.csv\n",
      "r/AmItheAsshole  2024-01\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-01_top50.csv\n",
      "r/AmItheAsshole  2024-02\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-02_top50.csv\n",
      "r/AmItheAsshole  2024-03\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-03_top50.csv\n",
      "r/AmItheAsshole  2024-04\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-04_top50.csv\n",
      "r/AmItheAsshole  2024-05\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-05_top50.csv\n",
      "r/AmItheAsshole  2024-06\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-06_top50.csv\n",
      "r/AmItheAsshole  2024-07\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-07_top50.csv\n",
      "r/AmItheAsshole  2024-08\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-08_top50.csv\n",
      "r/AmItheAsshole  2024-09\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-09_top50.csv\n",
      "r/AmItheAsshole  2024-10\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-10_top50.csv\n",
      "r/AmItheAsshole  2024-11\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-11_top50.csv\n",
      "r/AmItheAsshole  2024-12\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2024-12_top50.csv\n",
      "r/AmItheAsshole  2025-01\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-01_top50.csv\n",
      "r/AmItheAsshole  2025-02\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-02_top50.csv\n",
      "r/AmItheAsshole  2025-03\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-03_top50.csv\n",
      "r/AmItheAsshole  2025-04\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-04_top50.csv\n",
      "r/AmItheAsshole  2025-05\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-05_top50.csv\n",
      "r/AmItheAsshole  2025-06\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-06_top50.csv\n",
      "r/AmItheAsshole  2025-07\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-07_top50.csv\n",
      "r/AmItheAsshole  2025-08\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-08_top50.csv\n",
      "r/AmItheAsshole  2025-09\n",
      "  saved 50  ./raw_25month_range_data/AmItheAsshole/2025-09_top50.csv\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import os, glob, time, requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "subreddit_communities = [\"r/Science\", \"r/changemyview\", \"r/AmItheAsshole\"]\n",
    "API_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "data_dir = \"./raw_25month_range_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def fetch_posts(subreddit, after, before, limit=100):\n",
    "    params = {\n",
    "        \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "        \"after\": after,\n",
    "        \"before\": before,\n",
    "        \"limit\": limit,\n",
    "        \"sort\": \"desc\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(API_URL, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch err] {subreddit} {after}{before}: {e}\")\n",
    "        return []\n",
    "\n",
    "def minmax_norm(series):\n",
    "    smin, smax = series.min(), series.max()\n",
    "    if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "        return pd.Series([0.5] * len(series), index=series.index)\n",
    "    return (series - smin) / (smax - smin)\n",
    "\n",
    "def collect_month(subreddit, year, month):\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "    after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    seen, rows, cursor = set(), [], before\n",
    "    while True:\n",
    "        batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        for p in batch:\n",
    "            pid = p.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen.add(pid)\n",
    "                rows.append(p)\n",
    "\n",
    "        oldest_ts = min((p.get(\"created_utc\", 0) or 0) for p in batch)\n",
    "        if not oldest_ts:\n",
    "            break\n",
    "        cursor = datetime.fromtimestamp(float(oldest_ts)).strftime(\"%Y-%m-%d\")\n",
    "        if cursor <= after:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "    # popularity = 0.5 * norm(score) + 0.5 * norm(num_comments)\n",
    "    if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "        df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "        df[\"num_comments\"] = pd.to_numeric(df[\"num_comments\"], errors=\"coerce\")\n",
    "        df[\"score_norm\"] = minmax_norm(df[\"score\"].fillna(0))\n",
    "        df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].fillna(0))\n",
    "        df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "        df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "    else:\n",
    "        # fallback if fields missing\n",
    "        if \"score\" in df.columns:\n",
    "            df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").fillna(0)\n",
    "            df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.head(50).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def months_range(start_year, start_month, end_year, end_month):\n",
    "    y, m = start_year, start_month\n",
    "    while (y < end_year) or (y == end_year and m <= end_month):\n",
    "        yield y, m\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            y += 1\n",
    "            m = 1\n",
    "\n",
    "# Inclusive window: 2023-09 through 2025-09 (NOTE: this is 25 months total)\n",
    "START_Y, START_M = 2023, 9\n",
    "END_Y, END_M = 2025, 9\n",
    "\n",
    "for sub in subreddit_communities:\n",
    "    sub_name = sub.replace(\"r/\", \"\")\n",
    "    out_dir = os.path.join(data_dir, sub_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for y, m in months_range(START_Y, START_M, END_Y, END_M):\n",
    "        print(f\"{sub}  {y}-{m:02d}\")\n",
    "        dfm = collect_month(sub, y, m)\n",
    "        if dfm.empty:\n",
    "            print(\"  (no data)\")\n",
    "            continue\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "        cols = [\n",
    "            \"id\", \"title\", \"selftext\", \"author\", \"score\", \"num_comments\",\n",
    "            \"upvote_ratio\", \"popularity\", \"created_utc\", \"permalink\", \"year\", \"month\"\n",
    "        ]\n",
    "        keep = [c for c in cols if c in dfm.columns]\n",
    "        dfm[keep].to_csv(out_path, index=False)\n",
    "        print(f\"  saved {len(dfm)}  {out_path}\")"
=======
    "# import os, glob, time, requests\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# subreddit_communities = [\"r/Science\", \"r/changemyview\", \"r/AmItheAsshole\"]\n",
    "# API_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "# data_dir = \"./raw_25month_range_data\"\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# def fetch_posts(subreddit, after, before, limit=100):\n",
    "#     params = {\n",
    "#         \"subreddit\": subreddit.replace(\"r/\", \"\"),\n",
    "#         \"after\": after,\n",
    "#         \"before\": before,\n",
    "#         \"limit\": limit,\n",
    "#         \"sort\": \"desc\",\n",
    "#     }\n",
    "#     try:\n",
    "#         r = requests.get(API_URL, params=params, timeout=30)\n",
    "#         r.raise_for_status()\n",
    "#         return r.json().get(\"data\", [])\n",
    "#     except Exception as e:\n",
    "#         print(f\"[fetch err] {subreddit} {after}{before}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# def minmax_norm(series):\n",
    "#     smin, smax = series.min(), series.max()\n",
    "#     if pd.isna(smin) or pd.isna(smax) or smax == smin:\n",
    "#         return pd.Series([0.5] * len(series), index=series.index)\n",
    "#     return (series - smin) / (smax - smin)\n",
    "\n",
    "# def collect_month(subreddit, year, month):\n",
    "#     start = datetime(year, month, 1)\n",
    "#     end = datetime(year + (month == 12), (month % 12) + 1, 1)\n",
    "#     after, before = start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#     seen, rows, cursor = set(), [], before\n",
    "#     while True:\n",
    "#         batch = fetch_posts(subreddit, after, cursor, limit=100)\n",
    "#         if not batch:\n",
    "#             break\n",
    "\n",
    "#         for p in batch:\n",
    "#             pid = p.get(\"id\")\n",
    "#             if pid and pid not in seen:\n",
    "#                 seen.add(pid)\n",
    "#                 rows.append(p)\n",
    "\n",
    "#         oldest_ts = min((p.get(\"created_utc\", 0) or 0) for p in batch)\n",
    "#         if not oldest_ts:\n",
    "#             break\n",
    "#         cursor = datetime.fromtimestamp(float(oldest_ts)).strftime(\"%Y-%m-%d\")\n",
    "#         if cursor <= after:\n",
    "#             break\n",
    "\n",
    "#         time.sleep(1)\n",
    "\n",
    "#     if not rows:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     df[\"year\"], df[\"month\"] = year, month\n",
    "\n",
    "#     # popularity = 0.5 * norm(score) + 0.5 * norm(num_comments)\n",
    "#     if {\"score\", \"num_comments\"}.issubset(df.columns):\n",
    "#         df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "#         df[\"num_comments\"] = pd.to_numeric(df[\"num_comments\"], errors=\"coerce\")\n",
    "#         df[\"score_norm\"] = minmax_norm(df[\"score\"].fillna(0))\n",
    "#         df[\"comments_norm\"] = minmax_norm(df[\"num_comments\"].fillna(0))\n",
    "#         df[\"popularity\"] = 0.5 * df[\"score_norm\"] + 0.5 * df[\"comments_norm\"]\n",
    "#         df = df.sort_values(\"popularity\", ascending=False).head(50).reset_index(drop=True)\n",
    "#     else:\n",
    "#         # fallback if fields missing\n",
    "#         if \"score\" in df.columns:\n",
    "#             df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").fillna(0)\n",
    "#             df = df.sort_values(\"score\", ascending=False).head(50).reset_index(drop=True)\n",
    "#         else:\n",
    "#             df = df.head(50).reset_index(drop=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def months_range(start_year, start_month, end_year, end_month):\n",
    "#     y, m = start_year, start_month\n",
    "#     while (y < end_year) or (y == end_year and m <= end_month):\n",
    "#         yield y, m\n",
    "#         m += 1\n",
    "#         if m == 13:\n",
    "#             y += 1\n",
    "#             m = 1\n",
    "\n",
    "# # Inclusive window: 2023-09 through 2025-09 (NOTE: this is 25 months total)\n",
    "# START_Y, START_M = 2023, 9\n",
    "# END_Y, END_M = 2025, 9\n",
    "\n",
    "# for sub in subreddit_communities:\n",
    "#     sub_name = sub.replace(\"r/\", \"\")\n",
    "#     out_dir = os.path.join(data_dir, sub_name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     for y, m in months_range(START_Y, START_M, END_Y, END_M):\n",
    "#         print(f\"{sub}  {y}-{m:02d}\")\n",
    "#         dfm = collect_month(sub, y, m)\n",
    "#         if dfm.empty:\n",
    "#             print(\"  (no data)\")\n",
    "#             continue\n",
    "\n",
    "#         out_path = os.path.join(out_dir, f\"{y}-{m:02d}_top50.csv\")\n",
    "#         cols = [\n",
    "#             \"id\", \"title\", \"selftext\", \"author\", \"score\", \"num_comments\",\n",
    "#             \"upvote_ratio\", \"popularity\", \"created_utc\", \"permalink\", \"year\", \"month\"\n",
    "#         ]\n",
    "#         keep = [c for c in cols if c in dfm.columns]\n",
    "#         dfm[keep].to_csv(out_path, index=False)\n",
    "#         print(f\"  saved {len(dfm)}  {out_path}\")"
>>>>>>> origin/main
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### We want to train per-subreddit community, so create a Pandas dataframe (and CSV file) for each subreddit community that we can train our model based off of"
=======
    "#### We want to train per-subreddit community, so we created a Pandas dataframe (and CSV file) for each subreddit community that we can train our model based off of"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved  ./combined_raw_25month_range_data/Science_combined_24mo.csv (1250 rows)\n",
      "saved  ./combined_raw_25month_range_data/changemyview_combined_24mo.csv (1250 rows)\n",
      "saved  ./combined_raw_25month_range_data/AmItheAsshole_combined_24mo.csv (1250 rows)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "raw_data_dir = \"./raw_25month_range_data\"\n",
    "output_data_dir = \"./combined_raw_25month_range_data\"\n",
    "os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "def load_sub(sub):\n",
    "    files = sorted(glob.glob(os.path.join(raw_data_dir, sub, \"*.csv\")))\n",
    "    if not files:\n",
    "        print(f\"(no csvs) {sub}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat((pd.read_csv(f, low_memory=False) for f in files), ignore_index=True)\n",
    "    if \"subreddit\" not in df.columns:\n",
    "        df[\"subreddit\"] = sub\n",
    "    if \"id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=\"id\")\n",
    "    if \"created_utc\" in df.columns and \"created_datetime\" not in df.columns:\n",
    "        df[\"created_utc\"] = pd.to_numeric(df[\"created_utc\"], errors=\"coerce\")\n",
    "        df[\"created_datetime\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# build one DataFrame per subreddit\n",
    "df_science       = load_sub(\"Science\")\n",
    "df_changemyview  = load_sub(\"changemyview\")\n",
    "df_amitheasshole = load_sub(\"AmItheAsshole\")\n",
    "\n",
    "# save each to ./combined_subreddit_raw_data/\n",
    "outputs = {\n",
    "    \"Science\": df_science,\n",
    "    \"changemyview\": df_changemyview,\n",
    "    \"AmItheAsshole\": df_amitheasshole,\n",
    "}\n",
    "for name, df in outputs.items():\n",
    "    if df.empty:\n",
    "        print(f\"(skip empty) {name}\")\n",
    "        continue\n",
    "    out_path = os.path.join(output_data_dir, f\"{name}_combined_24mo.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"saved  {out_path} ({len(df)} rows)\")\n",
    "    \n",
    "# accounts for current month so collective of 25 months of data\n",
    "# currently, voting score and number of comments on a post have the same engagement weight (50%) - we can potentially determine better, more effecte weights "
=======
    "# raw_data_dir = \"./raw_25month_range_data\"\n",
    "# output_data_dir = \"./combined_raw_25month_range_data\"\n",
    "# os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "# def load_sub(sub):\n",
    "#     files = sorted(glob.glob(os.path.join(raw_data_dir, sub, \"*.csv\")))\n",
    "#     if not files:\n",
    "#         print(f\"(no csvs) {sub}\")\n",
    "#         return pd.DataFrame()\n",
    "#     df = pd.concat((pd.read_csv(f, low_memory=False) for f in files), ignore_index=True)\n",
    "#     if \"subreddit\" not in df.columns:\n",
    "#         df[\"subreddit\"] = sub\n",
    "#     if \"id\" in df.columns:\n",
    "#         df = df.drop_duplicates(subset=\"id\")\n",
    "#     if \"created_utc\" in df.columns and \"created_datetime\" not in df.columns:\n",
    "#         df[\"created_utc\"] = pd.to_numeric(df[\"created_utc\"], errors=\"coerce\")\n",
    "#         df[\"created_datetime\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\")\n",
    "#     return df\n",
    "\n",
    "# # build one DataFrame per subreddit\n",
    "# df_science       = load_sub(\"Science\")\n",
    "# df_changemyview  = load_sub(\"changemyview\")\n",
    "# df_amitheasshole = load_sub(\"AmItheAsshole\")\n",
    "\n",
    "# # save each to ./combined_subreddit_raw_data/\n",
    "# outputs = {\n",
    "#     \"Science\": df_science,\n",
    "#     \"changemyview\": df_changemyview,\n",
    "#     \"AmItheAsshole\": df_amitheasshole,\n",
    "# }\n",
    "# for name, df in outputs.items():\n",
    "#     if df.empty:\n",
    "#         print(f\"(skip empty) {name}\")\n",
    "#         continue\n",
    "#     out_path = os.path.join(output_data_dir, f\"{name}_combined_24mo.csv\")\n",
    "#     df.to_csv(out_path, index=False)\n",
    "#     print(f\"saved  {out_path} ({len(df)} rows)\")\n",
    "    \n",
    "# # accounts for current month so collective of 25 months of data\n",
    "# # currently, voting score and number of comments on a post have the same engagement weight (50%) - we can potentially determine better, more effecte weights "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I want to do a basic analysis. Starting with r/Science\n",
    "### We will be using \"./combined_raw_25month_range_data/Science_combine_24mo.csv\n",
    "### Let's Think: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->vaderSentiment) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->vaderSentiment) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->vaderSentiment) (2025.8.3)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40-cp312-cp312-macosx_10_13_universal2.whl.metadata (15 kB)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from bertopic) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from bertopic) (2.3.3)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting scikit-learn>=1.0 (from bertopic)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from bertopic) (4.67.1)\n",
      "Collecting llvmlite>0.36.0 (from bertopic)\n",
      "  Downloading llvmlite-0.45.1-cp312-cp312-macosx_10_15_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting scipy>=1.0 (from hdbscan>=0.8.29->bertopic)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from hdbscan>=0.8.29->bertopic) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Collecting narwhals>=1.15.1 (from plotly>=4.7.0->bertopic)\n",
      "  Downloading narwhals-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0->bertopic)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.62.1-cp312-cp312-macosx_10_15_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2025.8.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
      "Downloading hdbscan-0.8.40-cp312-cp312-macosx_10_13_universal2.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp312-cp312-macosx_10_15_x86_64.whl (43.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.0/43.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.7.0-py3-none-any.whl (412 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl (9.3 MB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl (23.6 MB)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl (182 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "Downloading numba-0.62.1-cp312-cp312-macosx_10_15_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-macosx_10_13_x86_64.whl (11 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing_extensions, threadpoolctl, sympy, scipy, safetensors, pyyaml, Pillow, networkx, narwhals, MarkupSafe, llvmlite, hf-xet, fsspec, filelock, scikit-learn, plotly, numba, jinja2, huggingface-hub, torch, tokenizers, pynndescent, hdbscan, umap-learn, transformers, sentence-transformers, bertopic\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m28/28\u001b[0m [bertopic]/28\u001b[0m [bertopic]transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 Pillow-11.3.0 bertopic-0.17.3 filelock-3.20.0 fsspec-2025.9.0 hdbscan-0.8.40 hf-xet-1.1.10 huggingface-hub-0.35.3 jinja2-3.1.6 llvmlite-0.45.1 mpmath-1.3.0 narwhals-2.7.0 networkx-3.5 numba-0.62.1 plotly-6.3.1 pynndescent-0.5.13 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-5.1.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.2.2 transformers-4.57.0 typing_extensions-4.15.0 umap-learn-0.5.9.post2\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-macosx_13_0_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Users/emilyho/.venvs/py312/lib/python3.12/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-macosx_13_0_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install vaderSentiment\n",
    "# !pip install bertopic\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned CSV saved to: ./combined_raw_25month_range_data/Science_combined_24mo_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1) Load the dataset ===\n",
    "input_path = \"./combined_raw_25month_range_data/Science_combined_24mo.csv\"\n",
    "output_path = \"./combined_raw_25month_range_data/Science_combined_24mo_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path, low_memory=False)\n",
    "\n",
    "# === 2) Basic cleaning function ===\n",
    "url_re   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "punct_re = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
    "space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.lower()\n",
    "    s = url_re.sub(\"\", s)\n",
    "    s = punct_re.sub(\" \", s)\n",
    "    s = space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# === 3) Apply cleaning to 'title' and text columns if present ===\n",
    "for col in [\"title\", \"post_title\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_text)\n",
    "\n",
    "for col in [\"selftext\", \"text\", \"body\", \"content\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_text)\n",
    "\n",
    "# === 4) Save cleaned version (keep all columns) ===\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\" Cleaned CSV saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === 6) Sentiment (VADER) ====================================================\n",
    "# vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def get_sentiment(text):\n",
    "#     s = vader.polarity_scores(text)[\"compound\"]\n",
    "#     return \"positive\" if s >= 0.05 else \"negative\" if s <= -0.05 else \"neutral\"\n",
    "\n",
    "# post_sentiment    = {pid: get_sentiment(txt) for pid, txt in clean_posts.items()}\n",
    "# comment_sentiment = {cid: get_sentiment(txt) for cid, txt in clean_comments.items()}\n",
    "\n",
    "# # Quick summary printouts\n",
    "# from collections import Counter\n",
    "# post_counts = Counter(post_sentiment.values())\n",
    "# comment_counts = Counter(comment_sentiment.values())\n",
    "\n",
    "# total_posts = len(post_sentiment)\n",
    "# total_comments = len(comment_sentiment)\n",
    "\n",
    "# print(\"Post Sentiment Distribution:\", post_counts)\n",
    "# print(\"Comment Sentiment Distribution:\", comment_counts)\n",
    "\n",
    "# print(\"\\nProportion of posts:\")\n",
    "# for k,v in post_counts.items():\n",
    "#     print(f\"  {k}: {v/total_posts:.2%}\")\n",
    "\n",
    "# print(\"\\nProportion of comments:\")\n",
    "# for k,v in comment_counts.items():\n",
    "#     print(f\"  {k}: {v/total_comments:.2%}\")\n",
    "\n",
    "# # === 7) Emotion model (HuggingFace) ==========================================\n",
    "# device_id = 0 if torch.cuda.is_available() else -1\n",
    "# print(f\"\\nUsing device: {'GPU' if device_id == 0 else 'CPU'}\")\n",
    "\n",
    "# emotion_model = pipeline(\n",
    "#     task=\"text-classification\",\n",
    "#     model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "#     top_k=None,\n",
    "#     device=device_id,\n",
    "#     truncation=True,\n",
    "#     padding=True,\n",
    "#     max_length=512\n",
    "# )\n",
    "\n",
    "# def dominant_emotion(text):\n",
    "#     text = text.strip()\n",
    "#     if not text:\n",
    "#         return \"none\"\n",
    "#     preds = emotion_model(text)[0]\n",
    "#     best = max(preds, key=lambda x: x[\"score\"])\n",
    "#     return best[\"label\"]\n",
    "\n",
    "# post_emotion    = {pid: dominant_emotion(txt) for pid, txt in clean_posts.items()}\n",
    "# comment_emotion = {cid: dominant_emotion(txt) for cid, txt in clean_comments.items()}\n",
    "\n",
    "# # === 8) (Optional) Plots ======================================================\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Sentiment bars\n",
    "# labels = list(post_counts.keys() | comment_counts.keys())\n",
    "# x = np.arange(len(labels))\n",
    "# width = 0.4\n",
    "\n",
    "# post_props = [post_counts.get(k,0)/max(1,total_posts) for k in labels]\n",
    "# comm_props = [comment_counts.get(k,0)/max(1,total_comments) for k in labels]\n",
    "\n",
    "# plt.bar(x - width/2, post_props, width=width, label=\"Posts\")\n",
    "# plt.bar(x + width/2, comm_props, width=width, label=\"Comments\")\n",
    "# plt.xticks(x, labels)\n",
    "# plt.ylabel(\"Proportion\")\n",
    "# plt.title(\"Sentiment Distribution: Posts vs Comments\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Emotion bars\n",
    "# post_counts_e = Counter(post_emotion.values())\n",
    "# comm_counts_e = Counter(comment_emotion.values())\n",
    "# emotions = list(post_counts_e.keys() | comm_counts_e.keys())\n",
    "# x = np.arange(len(emotions))\n",
    "\n",
    "# plt.bar(x - width/2, [post_counts_e.get(e,0)/max(1,len(post_emotion)) for e in emotions], width=width, label=\"Posts\")\n",
    "# plt.bar(x + width/2, [comm_counts_e.get(e,0)/max(1,len(comment_emotion)) for e in emotions], width=width, label=\"Comments\")\n",
    "# plt.xticks(x, emotions, rotation=45, ha=\"right\")\n",
    "# plt.ylabel(\"Proportion\")\n",
    "# plt.title(\"Emotion Distribution: Posts vs Comments\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Heatmap data (without seaborn; stays pure matplotlib-compatible if seaborn isn't installed)\n",
    "# # Build matrix: rows = post sentiment, cols = comment emotion\n",
    "# row_keys = [\"negative\",\"neutral\",\"positive\"]\n",
    "# col_keys = sorted(set(emotions + [\"none\"]))\n",
    "\n",
    "# heat = np.zeros((len(row_keys), len(col_keys)), dtype=int)\n",
    "# for pid in clean_posts:\n",
    "#     ps = post_sentiment[pid]\n",
    "#     for cid in post_comments_dict.get(pid, []):\n",
    "#         ce = comment_emotion.get(cid, \"none\")\n",
    "#         if ps in row_keys and ce in col_keys:\n",
    "#             heat[row_keys.index(ps), col_keys.index(ce)] += 1\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.imshow(heat, aspect=\"auto\")\n",
    "# plt.colorbar(label=\"Count\")\n",
    "# plt.xticks(np.arange(len(col_keys)), col_keys, rotation=45, ha=\"right\")\n",
    "# plt.yticks(np.arange(len(row_keys)), row_keys)\n",
    "# plt.title(\"Heatmap: Post Sentiment vs Comment Emotions\")\n",
    "# plt.xlabel(\"Comment Emotion\")\n",
    "# plt.ylabel(\"Post Sentiment\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # partition the data: training and testing sets\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your data\n",
    "# df = pd.read_csv(\"./combined_raw_25month_range_data/Science_combined_24mo.csv\")\n",
    "\n",
    "# # Split into training (2/3) and testing (1/3)\n",
    "# # this is a randomly chosen parition amount, we will research more about this later\n",
    "# train_df, test_df = train_test_split(df, test_size=1/3, random_state=42)\n",
    "\n",
    "\n",
    "\n"
>>>>>>> origin/main
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
