{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_bucket\n",
      "low       34344\n",
      "medium    17172\n",
      "high      17172\n",
      "Name: count, dtype: int64\n",
      "Bucket thresholds for log_score_cap:\n",
      "0.50    0.693147\n",
      "0.75    1.098612\n",
      "1.00    2.484907\n",
      "Name: log_score_cap, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\", low_memory=False)\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df[\"num_comments\"] = pd.to_numeric(df.get(\"num_comments\", np.nan), errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"score\",\"title\",\"text\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- cap the right tail; don't drop rows for final df ---\n",
    "q95 = df[\"score\"].quantile(0.95)\n",
    "df[\"log_score_cap\"] = np.log1p(df[\"score\"].clip(upper=q95))\n",
    "\n",
    "# percentile rank on capped variable; 'first' prevents ties collapsing medium\n",
    "df[\"log_score_pct\"] = df[\"log_score_cap\"].rank(method=\"first\", pct=True)\n",
    "\n",
    "# buckets: 0–50, 50–75, 75–100\n",
    "df[\"score_bucket\"] = pd.cut(\n",
    "    df[\"log_score_pct\"],\n",
    "    bins=[0, 0.50, 0.75, 1.0],\n",
    "    labels=[\"low\",\"medium\",\"high\"],\n",
    "    include_lowest=True, right=True\n",
    ")\n",
    "\n",
    "print(df[\"score_bucket\"].value_counts().reindex([\"low\",\"medium\",\"high\"]))\n",
    "thresholds = df[\"log_score_cap\"].quantile([0.5, 0.75, 1.0])\n",
    "print(\"Bucket thresholds for log_score_cap:\")\n",
    "print(thresholds)\n",
    "\n",
    "plt.hist(df[\"log_score_cap\"], bins=5)\n",
    "plt.title(\"Histogram of capped log_score\")\n",
    "plt.xlabel(\"log1p(score capped @ 95%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_category\n",
      "low       0.704315\n",
      "high      0.159338\n",
      "medium    0.136346\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# PUT BUCKET CATEGORY INTO NEW COLUMN\n",
    "# CLEAN THE DATA -- very briefly\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s?!.,:;']\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "original_df = pd.read_csv(\"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\", low_memory=False)\n",
    "clean_df = original_df.copy()\n",
    "\n",
    "clean_df[\"title\"] = clean_df[\"title\"].apply(clean_text)\n",
    "clean_df[\"text\"] = clean_df[\"text\"].apply(clean_text)\n",
    "clean_df = clean_df[(clean_df[\"title\"] != \"\") & (clean_df[\"text\"] != \"\")].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# low_bucket = thresholds.iloc[0]\n",
    "# medium_bucket = thresholds.iloc[1]\n",
    "# high_bucket = thresholds.iloc[2]\n",
    "\n",
    "clean_df[\"log_score\"] = np.log1p(clean_df[\"score\"])\n",
    "\n",
    "clean_df[\"bucket_category\"] = pd.qcut(\n",
    "    clean_df[\"log_score\"],\n",
    "    q=[0.0, 0.50, 0.75, 1.0],\n",
    "    labels=[\"low\",\"medium\",\"high\"]\n",
    ")\n",
    "print(clean_df[\"bucket_category\"].value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43868 | Val: 10968 | Test: 13710\n",
      "\n",
      "Train distribution:\n",
      " bucket_category\n",
      "low       0.704317\n",
      "high      0.159319\n",
      "medium    0.136364\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val distribution:\n",
      " bucket_category\n",
      "low       0.704322\n",
      "high      0.159373\n",
      "medium    0.136306\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      " bucket_category\n",
      "low       0.704303\n",
      "high      0.159373\n",
      "medium    0.136324\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    clean_df,\n",
    "    test_size=0.2,\n",
    "    stratify=clean_df[\"bucket_category\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df[\"bucket_category\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"\\nTrain distribution:\\n\", train_df[\"bucket_category\"].value_counts(normalize=True))\n",
    "print(\"\\nVal distribution:\\n\", val_df[\"bucket_category\"].value_counts(normalize=True))\n",
    "print(\"\\nTest distribution:\\n\", test_df[\"bucket_category\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43868 | Val: 10968 | Test: 13710\n",
      "\n",
      "Train distribution:\n",
      " bucket_category\n",
      "low       0.704317\n",
      "high      0.159319\n",
      "medium    0.136364\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val distribution:\n",
      " bucket_category\n",
      "low       0.704322\n",
      "high      0.159373\n",
      "medium    0.136306\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      " bucket_category\n",
      "low       0.704303\n",
      "high      0.159373\n",
      "medium    0.136324\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high      0.380     0.344     0.361      1748\n",
      "         low      0.743     0.876     0.804      7725\n",
      "      medium      0.179     0.033     0.056      1495\n",
      "\n",
      "    accuracy                          0.676     10968\n",
      "   macro avg      0.434     0.418     0.407     10968\n",
      "weighted avg      0.608     0.676     0.631     10968\n",
      "\n",
      "\n",
      "Test performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high      0.357     0.326     0.341      2185\n",
      "         low      0.741     0.874     0.802      9656\n",
      "      medium      0.169     0.030     0.051      1869\n",
      "\n",
      "    accuracy                          0.671     13710\n",
      "   macro avg      0.422     0.410     0.398     13710\n",
      "weighted avg      0.602     0.671     0.626     13710\n",
      "\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[ 712 1401   72]\n",
      " [1016 8437  203]\n",
      " [ 267 1546   56]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Prep target + text ---\n",
    "df = clean_df.dropna(subset=[\"bucket_category\"]).copy()\n",
    "df[\"bucket_category\"] = df[\"bucket_category\"].astype(str)\n",
    "\n",
    "# use title + text as the signal (or switch to just text if you prefer)\n",
    "df[\"text_all\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).str.strip()\n",
    "\n",
    "# --- Stratified splits ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text_all\"], df[\"bucket_category\"],\n",
    "    test_size=0.2, stratify=df[\"bucket_category\"], random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "print(\"\\nTrain distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nVal distribution:\\n\", y_val.value_counts(normalize=True))\n",
    "print(\"\\nTest distribution:\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# --- Text-only pipeline ---\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        ngram_range=(1, 2),   # unigrams + bigrams often help\n",
    "        min_df=5,             # ignore very rare terms\n",
    "        max_df=0.9,           # ignore extremely common terms\n",
    "        max_features=200_000  # cap vocab size to control memory\n",
    "    )),\n",
    "    (\"lr\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",  # handle class imbalance\n",
    "        multi_class=\"ovr\",\n",
    "        solver=\"liblinear\"        # solid for sparse, OVR setup\n",
    "    )),\n",
    "])\n",
    "\n",
    "# --- Train ---\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- Validate ---\n",
    "print(\"\\nValidation performance:\")\n",
    "y_val_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n",
    "\n",
    "# --- Test ---\n",
    "print(\"\\nTest performance:\")\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred, digits=3))\n",
    "\n",
    "print(\"\\nConfusion matrix (test):\")\n",
    "print(confusion_matrix(y_test, y_test_pred, labels=sorted(y_test.unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 0.5, 'tfidf__max_df': 0.9, 'tfidf__min_df': 10, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high      0.344     0.321     0.332      1748\n",
      "         low      0.742     0.820     0.779      7725\n",
      "      medium      0.164     0.088     0.115      1495\n",
      "\n",
      "    accuracy                          0.641     10968\n",
      "   macro avg      0.417     0.410     0.409     10968\n",
      "weighted avg      0.600     0.641     0.617     10968\n",
      "\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high      0.328     0.317     0.323      2185\n",
      "         low      0.744     0.819     0.779      9656\n",
      "      medium      0.173     0.090     0.118      1869\n",
      "\n",
      "    accuracy                          0.639     13710\n",
      "   macro avg      0.415     0.409     0.407     13710\n",
      "weighted avg      0.600     0.639     0.616     13710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = clean_df.dropna(subset=[\"bucket_category\"]).copy()\n",
    "df[\"y\"] = df[\"bucket_category\"].astype(str)\n",
    "df[\"text_all\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).str.strip()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text_all\"], df[\"y\"], test_size=0.2, stratify=df[\"y\"], random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        sublinear_tf=True,     # <-- big win for long-tail terms\n",
    "        ngram_range=(1,2)\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\"))  # stronger than LR on text\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__min_df\": [3, 5, 10],\n",
    "    \"tfidf__max_df\": [0.85, 0.9, 0.95],\n",
    "    \"tfidf__ngram_range\": [(1,2), (1,3)],\n",
    "    \"clf__C\": [0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(pipe, param_grid, scoring=\"f1_macro\", cv=cv, n_jobs=-1, verbose=0)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "best = gs.best_estimator_\n",
    "\n",
    "print(\"\\nValidation:\")\n",
    "print(classification_report(y_val, best.predict(X_val), digits=3))\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "print(classification_report(y_test, best.predict(X_test), digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original(68692, 13)\n",
      "(62412, 13)\n",
      "Skew (raw score):    89.53783383866588\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s?!.,:;']\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "file_path = \"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"original\" + str(df.shape))\n",
    "df[\"title\"] = df[\"title\"].apply(clean_text)\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "df = df[df[\"text\"] != \"\"]\n",
    "df = df[(df[\"score\"] != 0) & (df[\"num_comments\"] != 0)]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df[\"score\"].hist(bins=6)\n",
    "plt.title(\"Score\")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/2935761190.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  x = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/2935761190.py:36: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.769\n",
      "PR  AUC: 0.435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.723     0.802     13511\n",
      "           1      0.376     0.677     0.484      3336\n",
      "\n",
      "    accuracy                          0.714     16847\n",
      "   macro avg      0.639     0.700     0.643     16847\n",
      "weighted avg      0.797     0.714     0.739     16847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "\n",
    "# ---------- load & clean ----------\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\s?!.,:;']\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "df = pd.read_csv(\"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\", low_memory=False)\n",
    "\n",
    "for c in [\"score\",\"num_comments\"]:\n",
    "    df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
    "\n",
    "df[\"title\"] = df.get(\"title\",\"\").apply(clean_text)\n",
    "df[\"text\"]  = df.get(\"text\",\"\").apply(clean_text)\n",
    "df = df[(df[\"title\"]!=\"\") | (df[\"text\"]!=\"\")].dropna(subset=[\"score\"]).reset_index(drop=True)\n",
    "df[\"num_comments\"] = df[\"num_comments\"].fillna(0)\n",
    "\n",
    "# ---------- label: desirability (top 20% within month if possible) ----------\n",
    "def parse_ts(s):\n",
    "    x = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "    if x.notna().all(): return x\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return pd.to_datetime(num, unit=\"s\", utc=True, errors=\"coerce\")\n",
    "\n",
    "if \"created_utc\" in df.columns:\n",
    "    ts = parse_ts(df[\"created_utc\"])\n",
    "    df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
    "else:\n",
    "    df[\"ym\"] = \"ALL\"\n",
    "\n",
    "df[\"engagement_log\"] = np.log1p(df[\"score\"] + 0.5*df[\"num_comments\"])\n",
    "df[\"pct\"] = df.groupby(\"ym\")[\"engagement_log\"].rank(pct=True, method=\"average\")\n",
    "df[\"desirable\"] = (df[\"pct\"] >= 0.80).astype(int)\n",
    "\n",
    "# ---------- features ----------\n",
    "def combine_text(X):\n",
    "    return (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\")).values\n",
    "\n",
    "def ling_feats(X):\n",
    "    s = (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\"))\n",
    "    n_chars = s.str.len()\n",
    "    n_words = s.str.split().apply(len).astype(float)\n",
    "    avg_word_len = np.where(n_words>0, n_chars/n_words, 0)\n",
    "    exclam = s.str.count(\"!\")\n",
    "    qmark = s.str.count(r\"\\?\")\n",
    "    firstp = s.str.count(r\"\\b(i|me|my|mine|we|us|our|ours)\\b\")\n",
    "    secondp= s.str.count(r\"\\b(you|your|yours|u)\\b\")\n",
    "    numer  = s.str.count(r\"\\b\\d+(\\.\\d+)?\\b\")\n",
    "    return np.vstack([n_chars, n_words, avg_word_len, exclam, qmark, firstp, secondp, numer]).T\n",
    "\n",
    "txt_block = Pipeline([\n",
    "    (\"join\", FunctionTransformer(combine_text, validate=False)),\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95, max_features=50_000))\n",
    "])\n",
    "\n",
    "ling_block = Pipeline([\n",
    "    (\"fe\", FunctionTransformer(ling_feats, validate=False)),\n",
    "    (\"sc\", StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "feats = ColumnTransformer([\n",
    "    (\"txt\",  txt_block, [\"title\",\"text\"]),\n",
    "    (\"ling\", ling_block, [\"title\",\"text\"]),\n",
    "], sparse_threshold=0.3)\n",
    "\n",
    "model = LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    "\n",
    "pipe = Pipeline([(\"feats\", feats), (\"clf\", model)])\n",
    "\n",
    "# ---------- split (time-aware if possible) ----------\n",
    "groups = df[\"ym\"].astype(str)\n",
    "if groups.nunique() > 1:\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    tr_idx, te_idx = next(gss.split(df, df[\"desirable\"], groups=groups))\n",
    "    train, test = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "else:\n",
    "    train, test = train_test_split(df, test_size=0.2, stratify=df[\"desirable\"], random_state=42)\n",
    "\n",
    "X_train = train[[\"title\",\"text\"]]\n",
    "y_train = train[\"desirable\"].astype(int).values\n",
    "X_test  = test[[\"title\",\"text\"]]\n",
    "y_test  = test[\"desirable\"].astype(int).values\n",
    "\n",
    "# ---------- train & evaluate ----------\n",
    "pipe.fit(X_train, y_train)\n",
    "probs = pipe.predict_proba(X_test)[:,1]\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"ROC AUC:\", round(roc_auc_score(y_test, probs), 3))\n",
    "print(\"PR  AUC:\", round(average_precision_score(y_test, probs), 3))\n",
    "print(classification_report(y_test, preds, digits=3))\n",
    "\n",
    "# ---------- (optional) inspect most influential terms ----------\n",
    "# Uncomment to see top/bottom TF-IDF terms by weight\n",
    "# clf = pipe.named_steps[\"clf\"]\n",
    "# tfidf = pipe.named_steps[\"feats\"].named_transformers_[\"txt\"].named_steps[\"tfidf\"]\n",
    "# vocab = np.array(tfidf.get_feature_names_out())\n",
    "# coefs = clf.coef_[0][:len(vocab)]\n",
    "# print(\"Top + terms:\", vocab[np.argsort(coefs)][-20:][::-1])\n",
    "# print(\"Top - terms:\", vocab[np.argsort(coefs)][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/3417327612.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  x = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/3417327612.py:46: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score] R^2=0.026 | RMSE_log=0.860 | RMSE=84.153\n",
      "[Comments] R^2=0.193 | RMSE_log=1.049 | RMSE=54.303\n",
      "[High-Score (top 10%)] ROC-AUC=0.765 | PR-AUC=0.304\n",
      "[Low-Score (bottom 10%)] ROC-AUC=0.569 | PR-AUC=0.286\n",
      "[High-Comments (top 10%)] ROC-AUC=0.796 | PR-AUC=0.316\n",
      "[Low-Comments (bottom 10%)] ROC-AUC=0.748 | PR-AUC=0.690\n",
      "\n",
      "Top 5 predicted high-score posts:\n",
      "                                                   title   score  pred_score  \\\n",
      "39090  i don't understand why i've never been fired f...     1.0    8.192509   \n",
      "36830           how do i tell my boss that i m quitting?   218.0    8.233465   \n",
      "46930  i m afraid to tell my boss that the workload i...     8.0   10.092512   \n",
      "53669  seriously considering leaving my wfh job for a...  1047.0    6.024105   \n",
      "53339                                      time to quit?     1.0    5.536120   \n",
      "\n",
      "       p_high_score  \n",
      "39090      0.966428  \n",
      "36830      0.963489  \n",
      "46930      0.958113  \n",
      "53669      0.957757  \n",
      "53339      0.956527  \n",
      "\n",
      "Top 5 predicted high-comment posts:\n",
      "                                                   title  num_comments  \\\n",
      "53210                                fired on my day off          38.0   \n",
      "46930  i m afraid to tell my boss that the workload i...          17.0   \n",
      "38914  my boss refused to put me on a pip and strongl...          95.0   \n",
      "36830           how do i tell my boss that i m quitting?         158.0   \n",
      "37210  is my boyfriend supportive in me making more m...           9.0   \n",
      "\n",
      "       pred_comments  p_high_comm  \n",
      "53210      37.368985     0.978792  \n",
      "46930      28.817621     0.978378  \n",
      "38914      24.342205     0.968637  \n",
      "36830      38.053144     0.965169  \n",
      "37210      17.318673     0.961102  \n",
      "\n",
      "Likely low-score posts:\n",
      "                                                   title  score  pred_score  \\\n",
      "37026  cracking the sap c ts452 2022 certification: k...    0.0    1.600227   \n",
      "57281  make your career with ibm c1000 068 certification    0.0    1.164732   \n",
      "36935  sap c thr89 2305 certification exam guide and ...    2.0    0.822100   \n",
      "57178  ibm c1000 168 certification exam: sample quest...    1.0    0.938400   \n",
      "56970  ibm c1000 151 certification exam: how to pass ...    1.0    0.774640   \n",
      "\n",
      "       p_low_score  \n",
      "37026     0.984914  \n",
      "57281     0.984333  \n",
      "36935     0.983508  \n",
      "57178     0.982185  \n",
      "56970     0.978956  \n",
      "\n",
      "Likely low-comment posts:\n",
      "                     title  num_comments  pred_comments  p_low_comm\n",
      "19192  dragon skulltiddies           0.0            0.0    1.000000\n",
      "21524                                0.0            0.0    0.999999\n",
      "18692                                0.0            0.0    0.999999\n",
      "52550                                0.0            0.0    0.999999\n",
      "52545                                1.0            0.0    0.999999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error,\n",
    "    roc_auc_score, average_precision_score, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) Load & clean\n",
    "# =========================\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\s?!.,:;']\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "df = pd.read_csv(\"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\", low_memory=False)\n",
    "\n",
    "for c in [\"score\",\"num_comments\"]:\n",
    "    df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
    "\n",
    "df[\"title\"] = df.get(\"title\",\"\").apply(clean_text)\n",
    "df[\"text\"]  = df.get(\"text\",\"\").apply(clean_text)\n",
    "df = df[(df[\"title\"]!=\"\") | (df[\"text\"]!=\"\")].dropna(subset=[\"score\",\"num_comments\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 1) Time key (for leakage-safe splits & per-month percentiles)\n",
    "# =========================\n",
    "def parse_ts(s):\n",
    "    # robustly parse created_utc if present; supports ISO or epoch seconds\n",
    "    x = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "    if x.notna().all():\n",
    "        return x\n",
    "    # fall back to epoch seconds\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return pd.to_datetime(num, unit=\"s\", utc=True, errors=\"coerce\")\n",
    "\n",
    "if \"created_utc\" in df.columns:\n",
    "    ts = parse_ts(df[\"created_utc\"])\n",
    "    df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
    "else:\n",
    "    df[\"ym\"] = \"ALL\"\n",
    "\n",
    "# =========================\n",
    "# 2) Feature pipeline (shared)\n",
    "# =========================\n",
    "def combine_text(X):\n",
    "    return (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\")).values\n",
    "\n",
    "def ling_feats(X):\n",
    "    s = (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\"))\n",
    "    n_chars = s.str.len()\n",
    "    n_words = s.str.split().apply(len).astype(float)\n",
    "    avg_word_len = np.where(n_words>0, n_chars/n_words, 0)\n",
    "    exclam = s.str.count(\"!\")\n",
    "    qmark = s.str.count(r\"\\?\")\n",
    "    firstp = s.str.count(r\"\\b(i|me|my|mine|we|us|our|ours)\\b\")\n",
    "    secondp= s.str.count(r\"\\b(you|your|yours|u)\\b\")\n",
    "    numer  = s.str.count(r\"\\b\\d+(\\.\\d+)?\\b\")\n",
    "    return np.vstack([n_chars, n_words, avg_word_len, exclam, qmark, firstp, secondp, numer]).T\n",
    "\n",
    "txt_block = Pipeline([\n",
    "    (\"join\", FunctionTransformer(combine_text, validate=False)),\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95, max_features=50_000))\n",
    "])\n",
    "\n",
    "ling_block = Pipeline([\n",
    "    (\"fe\", FunctionTransformer(ling_feats, validate=False)),\n",
    "    (\"sc\", StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "feats = ColumnTransformer([\n",
    "    (\"txt\",  txt_block, [\"title\",\"text\"]),\n",
    "    (\"ling\", ling_block, [\"title\",\"text\"]),\n",
    "], sparse_threshold=0.3)\n",
    "\n",
    "# =========================\n",
    "# 3) Targets\n",
    "#    - Regressions on log1p(score) and log1p(num_comments)\n",
    "#    - Binary labels for extremes (per month): top 10% (high) and bottom 10% (low)\n",
    "# =========================\n",
    "df[\"score_log\"] = np.log1p(df[\"score\"].clip(lower=0))\n",
    "df[\"comments_log\"] = np.log1p(df[\"num_comments\"].clip(lower=0))\n",
    "\n",
    "hi_q, lo_q = 0.90, 0.10  # \"very well\" / \"very poorly\" within month\n",
    "\n",
    "def per_group_flags(s, group, hi=0.90, lo=0.10):\n",
    "    q_hi = s.groupby(group).transform(lambda g: g.quantile(hi))\n",
    "    q_lo = s.groupby(group).transform(lambda g: g.quantile(lo))\n",
    "    return (s >= q_hi).astype(int), (s <= q_lo).astype(int)\n",
    "\n",
    "high_score, low_score = per_group_flags(df[\"score\"], df[\"ym\"], hi_q, lo_q)\n",
    "high_comm,  low_comm  = per_group_flags(df[\"num_comments\"], df[\"ym\"], hi_q, lo_q)\n",
    "\n",
    "df[\"high_score\"] = high_score\n",
    "df[\"low_score\"]  = low_score\n",
    "df[\"high_comm\"]  = high_comm\n",
    "df[\"low_comm\"]   = low_comm\n",
    "\n",
    "# =========================\n",
    "# 4) Train/test split (time-aware if multiple months)\n",
    "# =========================\n",
    "groups = df[\"ym\"].astype(str)\n",
    "if groups.nunique() > 1:\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    tr_idx, te_idx = next(gss.split(df, None, groups=groups))\n",
    "    train, test = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "else:\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[[\"high_score\",\"high_comm\"]])\n",
    "\n",
    "X_train = train[[\"title\",\"text\"]]\n",
    "X_test  = test[[\"title\",\"text\"]]\n",
    "\n",
    "# =========================\n",
    "# 5) Models  (sparse-friendly)\n",
    "# =========================\n",
    "# Regressors (continuous, sparse OK)\n",
    "reg_score = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"reg\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "reg_comm = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"reg\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# Classifiers (extremes, sparse OK)\n",
    "clf_hi_score = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
    "])\n",
    "clf_lo_score = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
    "])\n",
    "clf_hi_comm = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
    "])\n",
    "clf_lo_comm = Pipeline([\n",
    "    (\"feats\", feats),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 6) Fit\n",
    "# =========================\n",
    "# Regressions\n",
    "y_score_log_tr = train[\"score_log\"]\n",
    "y_comm_log_tr  = train[\"comments_log\"]\n",
    "\n",
    "reg_score.fit(X_train, y_score_log_tr)\n",
    "reg_comm.fit(X_train,  y_comm_log_tr)\n",
    "\n",
    "# Classifiers\n",
    "y_hi_score_tr = train[\"high_score\"].astype(int)\n",
    "y_lo_score_tr = train[\"low_score\"].astype(int)\n",
    "y_hi_comm_tr  = train[\"high_comm\"].astype(int)\n",
    "y_lo_comm_tr  = train[\"low_comm\"].astype(int)\n",
    "\n",
    "clf_hi_score.fit(X_train, y_hi_score_tr)\n",
    "clf_lo_score.fit(X_train, y_lo_score_tr)\n",
    "clf_hi_comm.fit(X_train,  y_hi_comm_tr)\n",
    "clf_lo_comm.fit(X_train,  y_lo_comm_tr)\n",
    "\n",
    "# =========================\n",
    "# 7) Evaluate\n",
    "# =========================\n",
    "# Regressions\n",
    "y_score_log_te = test[\"score_log\"].values\n",
    "y_comm_log_te  = test[\"comments_log\"].values\n",
    "\n",
    "pred_score_log = reg_score.predict(X_test)\n",
    "pred_comm_log  = reg_comm.predict(X_test)\n",
    "\n",
    "def report_reg(name, y_true_log, y_pred_log):\n",
    "    r2  = r2_score(y_true_log, y_pred_log)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    # also compute RMSE in original space\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log.clip(min=0))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"[{name}] R^2={r2:.3f} | RMSE_log={rmse_log:.3f} | RMSE={rmse:.3f}\")\n",
    "\n",
    "report_reg(\"Score\",    y_score_log_te, pred_score_log)\n",
    "report_reg(\"Comments\", y_comm_log_te,  pred_comm_log)\n",
    "\n",
    "# Classifiers\n",
    "y_hi_score_te = test[\"high_score\"].astype(int).values\n",
    "y_lo_score_te = test[\"low_score\"].astype(int).values\n",
    "y_hi_comm_te  = test[\"high_comm\"].astype(int).values\n",
    "y_lo_comm_te  = test[\"low_comm\"].astype(int).values\n",
    "\n",
    "probs_hi_score = clf_hi_score.predict_proba(X_test)[:,1]\n",
    "probs_lo_score = clf_lo_score.predict_proba(X_test)[:,1]\n",
    "probs_hi_comm  = clf_hi_comm.predict_proba(X_test)[:,1]\n",
    "probs_lo_comm  = clf_lo_comm.predict_proba(X_test)[:,1]\n",
    "\n",
    "def report_clf(name, y_true, probs):\n",
    "    auc = roc_auc_score(y_true, probs)\n",
    "    ap  = average_precision_score(y_true, probs)\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f}\")\n",
    "\n",
    "report_clf(\"High-Score (top 10%)\", y_hi_score_te, probs_hi_score)\n",
    "report_clf(\"Low-Score (bottom 10%)\", y_lo_score_te, probs_lo_score)\n",
    "report_clf(\"High-Comments (top 10%)\", y_hi_comm_te, probs_hi_comm)\n",
    "report_clf(\"Low-Comments (bottom 10%)\", y_lo_comm_te, probs_lo_comm)\n",
    "\n",
    "# (Optional) hard labels at 0.5 threshold\n",
    "# print(classification_report(y_hi_score_te, (probs_hi_score>=0.5).astype(int), digits=3))\n",
    "\n",
    "# =========================\n",
    "# 8) Consolidate predictions for inspection / ranking\n",
    "# =========================\n",
    "test_results = test[[\"title\",\"text\",\"score\",\"num_comments\",\"ym\"]].copy()\n",
    "\n",
    "# continuous predictions (original scale)\n",
    "test_results[\"pred_score\"]    = np.expm1(np.maximum(pred_score_log, 0))\n",
    "test_results[\"pred_comments\"] = np.expm1(np.maximum(pred_comm_log, 0))\n",
    "\n",
    "# extreme probabilities\n",
    "test_results[\"p_high_score\"] = probs_hi_score\n",
    "test_results[\"p_low_score\"]  = probs_lo_score\n",
    "test_results[\"p_high_comm\"]  = probs_hi_comm\n",
    "test_results[\"p_low_comm\"]   = probs_lo_comm\n",
    "\n",
    "# convenience flags (you can tune thresholds)\n",
    "test_results[\"pred_high_score_flag\"] = (test_results[\"p_high_score\"] >= 0.5).astype(int)\n",
    "test_results[\"pred_low_score_flag\"]  = (test_results[\"p_low_score\"]  >= 0.5).astype(int)\n",
    "test_results[\"pred_high_comm_flag\"]  = (test_results[\"p_high_comm\"]  >= 0.5).astype(int)\n",
    "test_results[\"pred_low_comm_flag\"]   = (test_results[\"p_low_comm\"]   >= 0.5).astype(int)\n",
    "\n",
    "# Peek a few examples (comment/uncomment as needed)\n",
    "print(\"\\nTop 5 predicted high-score posts:\")\n",
    "print(test_results.sort_values(\"p_high_score\", ascending=False)[[\"title\",\"score\",\"pred_score\",\"p_high_score\"]].head(5))\n",
    "\n",
    "print(\"\\nTop 5 predicted high-comment posts:\")\n",
    "print(test_results.sort_values(\"p_high_comm\", ascending=False)[[\"title\",\"num_comments\",\"pred_comments\",\"p_high_comm\"]].head(5))\n",
    "\n",
    "print(\"\\nLikely low-score posts:\")\n",
    "print(test_results.sort_values(\"p_low_score\", ascending=False)[[\"title\",\"score\",\"pred_score\",\"p_low_score\"]].head(5))\n",
    "\n",
    "print(\"\\nLikely low-comment posts:\")\n",
    "print(test_results.sort_values(\"p_low_comm\", ascending=False)[[\"title\",\"num_comments\",\"pred_comments\",\"p_low_comm\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hold-out test performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high      0.409     0.422     0.416      4579\n",
      "         low      0.417     0.420     0.418      4579\n",
      "      medium      0.407     0.390     0.398      4580\n",
      "\n",
      "    accuracy                          0.411     13738\n",
      "   macro avg      0.411     0.411     0.411     13738\n",
      "weighted avg      0.411     0.411     0.411     13738\n",
      "\n",
      "\n",
      "POST: Laid off last week—how should I approach salary negotiations now?...\n",
      "Predicted bucket: high\n",
      "\n",
      "POST: Passed Series 7! Any advice for entry-level roles in wealth management?...\n",
      "Predicted bucket: low\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "data = df.dropna(subset=[\"text\", \"bucket\"]).copy()\n",
    "X = data[\"text\"].astype(str).values\n",
    "y = data[\"bucket\"].astype(str).values  # 'low'/'medium'/'high'\n",
    "\n",
    "# --- 2) Train/test split (hold out a portion for checking) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        sublinear_tf=True,\n",
    "        min_df=5,\n",
    "        ngram_range=(1,2)\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"\\nHold-out test performance:\")\n",
    "print(classification_report(y_test, pipe.predict(X_test), digits=3))\n",
    "\n",
    "\n",
    "def predict_bucket(texts, model_path=\"bucket_text_svc.joblib\"):\n",
    "    model = load(model_path)\n",
    "    preds = model.predict(list(map(str, texts)))\n",
    "    # If you want per-class margins (confidence-ish):\n",
    "    margins = model.decision_function(list(map(str, texts)))\n",
    "    return preds, margins\n",
    "\n",
    "# Example:\n",
    "future_posts = [\n",
    "    \"Laid off last week—how should I approach salary negotiations now?\",\n",
    "    \"Passed Series 7! Any advice for entry-level roles in wealth management?\"\n",
    "]\n",
    "pred_labels, margins = predict_bucket(future_posts)\n",
    "for t, p in zip(future_posts, pred_labels):\n",
    "    print(f\"\\nPOST: {t[:80]}...\")\n",
    "    print(f\"Predicted bucket: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/1518958948.py:34: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(df.get(\"created_utc\"), utc=True, errors=\"coerce\")\n",
      "/var/folders/pc/4kh3334956x9gt9qq4g7n9mh0000gn/T/ipykernel_7699/1518958948.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emilyho/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score] R2=0.083 | RMSE_log=0.945 | RMSE=110.3\n",
      "[Comments] R2=0.143 | RMSE_log=0.982 | RMSE=56.8\n",
      "[High-Score (top 5%)] ROC-AUC=0.815 | PR-AUC=0.208\n",
      "[Low-Score (bottom 5%)] ROC-AUC=0.618 | PR-AUC=0.612\n",
      "[High-Comments (top 5%)] ROC-AUC=0.811 | PR-AUC=0.218\n",
      "[Low-Comments (bottom 5%)] ROC-AUC=0.622 | PR-AUC=0.368\n",
      "\n",
      "Top 5 predicted high-score posts:\n",
      "                                                   title  score  pred_score  \\\n",
      "10646  i make 150k a year without a degree in a non s...    8.0   52.042334   \n",
      "18870  how to lose a job in 10 days, is this really t...    1.0   17.823911   \n",
      "35989  there's no such thing as a dream job. it's jus...   18.0   17.015980   \n",
      "9866   how i found a 100k supply chain job using job ...   96.0   69.572647   \n",
      "8084   where i work, i feel like people generally ign...    9.0   25.719503   \n",
      "\n",
      "       p_high_score  \n",
      "10646      0.993848  \n",
      "18870      0.975160  \n",
      "35989      0.966524  \n",
      "9866       0.964269  \n",
      "8084       0.944428  \n",
      "\n",
      "Top 5 predicted high-comment posts:\n",
      "                                                   title  num_comments  \\\n",
      "16679              denied a raise because of my age? 26f         103.0   \n",
      "25332        company wants me to work ten hours for free          80.0   \n",
      "26754  if a lot of people in a department seem bad at...           3.0   \n",
      "17174  would you burn bridge with employer that screw...          13.0   \n",
      "26835  is my boss reaction to my resignation notice n...         106.0   \n",
      "\n",
      "       pred_comments  p_high_comm  \n",
      "16679      33.258123     0.937784  \n",
      "25332      41.189947     0.933762  \n",
      "26754      42.515998     0.925183  \n",
      "17174      26.360656     0.919156  \n",
      "26835      27.508604     0.916744  \n"
     ]
    }
   ],
   "source": [
    "# Reddit post performance: simple local model (one subreddit)\n",
    "# - Shared text features (TF-IDF + a few counts)\n",
    "# - 2 regressors: predict score, num_comments (log1p)\n",
    "# - 4 classifiers: top/bottom 10% for score and comments\n",
    "# Written to be readable, not fancy :)\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, roc_auc_score, average_precision_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load + light cleaning\n",
    "# -----------------------------\n",
    "def clean_text(x: str) -> str:\n",
    "    x = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", str(x).lower())\n",
    "    x = re.sub(r\"[^a-z0-9\\s?!.,:;']\", \" \", x)\n",
    "    return re.sub(r\"\\s+\", \" \", x).strip()\n",
    "\n",
    "df = pd.read_csv(\"./updated_data_rp3/data/careeradvice/combined_careeradvice_raw.csv\", low_memory=False)\n",
    "df[\"title\"] = df.get(\"title\", \"\").apply(clean_text)\n",
    "df[\"text\"]  = df.get(\"text\", \"\").apply(clean_text)\n",
    "df[\"score\"] = pd.to_numeric(df.get(\"score\"), errors=\"coerce\")\n",
    "df[\"num_comments\"] = pd.to_numeric(df.get(\"num_comments\"), errors=\"coerce\")\n",
    "df = df[(df[\"title\"] != \"\") | (df[\"text\"] != \"\")]\n",
    "df = df[(df[\"num_comments\"] != 0) & (df[\"score\"] != 0)]\n",
    "df = df.dropna(subset=[\"score\", \"num_comments\"]).reset_index(drop=True)\n",
    "\n",
    "# month key (if we have timestamps)\n",
    "ts = pd.to_datetime(df.get(\"created_utc\"), utc=True, errors=\"coerce\")\n",
    "df[\"ym\"] = ts.dt.to_period(\"M\").astype(str).fillna(\"ALL\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Shared features\n",
    "# -----------------------------\n",
    "def join_text(X):\n",
    "    return (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\")).values\n",
    "\n",
    "def simple_ling_feats(X):\n",
    "    s = (X[\"title\"].fillna(\"\") + \" \" + X[\"text\"].fillna(\"\"))\n",
    "    n = s.str.len()\n",
    "    w = s.str.split().apply(len).astype(float)\n",
    "    feats = np.vstack([\n",
    "        n,                                  # n_chars\n",
    "        w,                                  # n_words\n",
    "        np.where(w > 0, n / w, 0),          # avg_word_len\n",
    "        s.str.count(\"!\"),                   # exclamations\n",
    "        s.str.count(r\"\\?\"),                 # question marks\n",
    "        s.str.count(r\"\\b(i|me|my|we|us|our|ours)\\b\"),    # 1st person\n",
    "        s.str.count(r\"\\b(you|your|yours|u)\\b\"),          # 2nd person\n",
    "        s.str.count(r\"\\b\\d+(\\.\\d+)?\\b\")     # numbers\n",
    "    ]).T\n",
    "    return feats\n",
    "\n",
    "feats = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"txt\", Pipeline([\n",
    "            (\"join\", FunctionTransformer(join_text, validate=False)),\n",
    "            (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.95, max_features=50_000))\n",
    "        ]), [\"title\", \"text\"]),\n",
    "        (\"ling\", Pipeline([\n",
    "            (\"make\", FunctionTransformer(simple_ling_feats, validate=False)),\n",
    "            (\"scale\", StandardScaler(with_mean=False))\n",
    "        ]), [\"title\", \"text\"]),\n",
    "    ],\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Targets\n",
    "# -----------------------------\n",
    "# continuous (log1p so huge posts don't dominate)\n",
    "df[\"score_log\"] = np.log1p(df[\"score\"].clip(lower=0))\n",
    "df[\"comments_log\"] = np.log1p(df[\"num_comments\"].clip(lower=0))\n",
    "\n",
    "# extremes per month (top/bottom 10%)\n",
    "HI, LO = 0.95, 0.05\n",
    "def hi_lo_flags(s, group_col, hi=HI, lo=LO):\n",
    "    q_hi = s.groupby(group_col).transform(lambda g: g.quantile(hi))\n",
    "    q_lo = s.groupby(group_col).transform(lambda g: g.quantile(lo))\n",
    "    return (s >= q_hi).astype(int), (s <= q_lo).astype(int)\n",
    "\n",
    "df[\"high_score\"], df[\"low_score\"] = hi_lo_flags(df[\"score\"], df[\"ym\"])\n",
    "df[\"high_comm\"],  df[\"low_comm\"]  = hi_lo_flags(df[\"num_comments\"], df[\"ym\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Split (time-aware if possible)\n",
    "# -----------------------------\n",
    "if df[\"ym\"].nunique() > 1:\n",
    "    gss = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
    "    tr_idx, te_idx = next(gss.split(df, groups=df[\"ym\"]))\n",
    "    train, test = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "else:\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[[\"high_score\",\"high_comm\"]])\n",
    "\n",
    "Xtr, Xte = train[[\"title\", \"text\"]], test[[\"title\", \"text\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Models (sparse-friendly)\n",
    "# -----------------------------\n",
    "# regressors\n",
    "reg_score = Pipeline([(\"feats\", feats), (\"reg\", Ridge(alpha=1.0))])\n",
    "reg_comm  = Pipeline([(\"feats\", feats), (\"reg\", Ridge(alpha=1.0))])\n",
    "\n",
    "# same classifier factory for all four “extreme” tasks\n",
    "def make_clf():\n",
    "    return Pipeline([(\"feats\", feats),\n",
    "                     (\"clf\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))])\n",
    "\n",
    "clf_hi_score = make_clf()\n",
    "clf_lo_score = make_clf()\n",
    "clf_hi_comm  = make_clf()\n",
    "clf_lo_comm  = make_clf()\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fit\n",
    "# -----------------------------\n",
    "reg_score.fit(Xtr, train[\"score_log\"])\n",
    "reg_comm.fit(Xtr,  train[\"comments_log\"])\n",
    "\n",
    "clf_hi_score.fit(Xtr, train[\"high_score\"])\n",
    "clf_lo_score.fit(Xtr, train[\"low_score\"])\n",
    "clf_hi_comm.fit(Xtr,  train[\"high_comm\"])\n",
    "clf_lo_comm.fit(Xtr,  train[\"low_comm\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Evaluate (short + readable)\n",
    "# -----------------------------\n",
    "from sklearn import __version__ as skl_version\n",
    "\n",
    "def reg_report(name, y_true_log, y_pred_log):\n",
    "    # Ensure numpy arrays (avoids weird dtype issues)\n",
    "    y_true_log = np.asarray(y_true_log)\n",
    "    y_pred_log = np.asarray(y_pred_log)\n",
    "\n",
    "    r2 = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # Older sklearn: no 'squared' kwarg → compute RMSE by hand\n",
    "    mse_log = mean_squared_error(y_true_log, y_pred_log)\n",
    "    rmse_log = np.sqrt(mse_log)\n",
    "\n",
    "    # Also report RMSE back on the original scale\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(np.maximum(y_pred_log, 0))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"[{name}] R2={r2:.3f} | RMSE_log={rmse_log:.3f} | RMSE={rmse:.1f}\")\n",
    "\n",
    "reg_report(\"Score\",    test[\"score_log\"],    reg_score.predict(Xte))\n",
    "reg_report(\"Comments\", test[\"comments_log\"], reg_comm.predict(Xte))\n",
    "\n",
    "def clf_report(name, y_true, probs):\n",
    "    print(f\"[{name}] ROC-AUC={roc_auc_score(y_true, probs):.3f} | PR-AUC={average_precision_score(y_true, probs):.3f}\")\n",
    "\n",
    "p_hi_score = clf_hi_score.predict_proba(Xte)[:, 1]\n",
    "p_lo_score = clf_lo_score.predict_proba(Xte)[:, 1]\n",
    "p_hi_comm  = clf_hi_comm.predict_proba(Xte)[:, 1]\n",
    "p_lo_comm  = clf_lo_comm.predict_proba(Xte)[:, 1]\n",
    "\n",
    "clf_report(\"High-Score (top 5%)\",     test[\"high_score\"], p_hi_score)\n",
    "clf_report(\"Low-Score (bottom 5%)\",   test[\"low_score\"],  p_lo_score)\n",
    "clf_report(\"High-Comments (top 5%)\",  test[\"high_comm\"],  p_hi_comm)\n",
    "clf_report(\"Low-Comments (bottom 5%)\",test[\"low_comm\"],   p_lo_comm)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Quick peek at predictions for sanity\n",
    "# -----------------------------\n",
    "results = test[[\"title\",\"score\",\"num_comments\",\"ym\"]].copy()\n",
    "results[\"pred_score\"]    = np.expm1(np.maximum(reg_score.predict(Xte), 0))\n",
    "results[\"pred_comments\"] = np.expm1(np.maximum(reg_comm.predict(Xte), 0))\n",
    "results[\"p_high_score\"]  = p_hi_score\n",
    "results[\"p_high_comm\"]   = p_hi_comm\n",
    "results[\"p_low_score\"]   = p_lo_score\n",
    "results[\"p_low_comm\"]    = p_lo_comm\n",
    "\n",
    "print(\"\\nTop 5 predicted high-score posts:\")\n",
    "print(results.sort_values(\"p_high_score\", ascending=False)[[\"title\",\"score\",\"pred_score\",\"p_high_score\"]].head(5))\n",
    "\n",
    "print(\"\\nTop 5 predicted high-comment posts:\")\n",
    "print(results.sort_values(\"p_high_comm\", ascending=False)[[\"title\",\"num_comments\",\"pred_comments\",\"p_high_comm\"]].head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "602039c487ccb8916c4ce42797e8a242a3fe8d1e182433a2fc68388a402f1cf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
